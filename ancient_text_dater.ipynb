{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are some common modules used in machine learning for importing/modifying data as well as visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the encoded_df_blanks_as_na.csv file for our features dataframe. features_df contains the independent variables used to train our machine learning model. These are the characterisitcs of the data that our model will analyze to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- A one-hot encoded matrix of features, with 11599 person names (nam_id_XXXX)\n",
    "    and 4784 place names (geo_id_XXXX) as columns.\n",
    "- The first column contains unique text_id corresponding to individual texts.\n",
    "'''\n",
    "features_df = pd.read_csv('encoded_df_blanks_as_na.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the 20240103_texts_with_dates.csv file for our labels dataframe. We then preview the dataframe. labels_df contains the dependent variables (target values) our model is trying to predict. These are the outputs corresponding to observations in the features_df. y1 and y1 willl serve as the ground truth our model learns to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tex_id</th>\n",
       "      <th>geotex_id</th>\n",
       "      <th>written</th>\n",
       "      <th>found</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>language_text</th>\n",
       "      <th>material_text</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>remark</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12042</td>\n",
       "      <td>8388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>117.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y1 = earliest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12054</td>\n",
       "      <td>8391</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y2 = latest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12063</td>\n",
       "      <td>8393</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[if y1 = y2, then we are certain of the date]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12064</td>\n",
       "      <td>8394</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17239</td>\n",
       "      <td>9507</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29245</th>\n",
       "      <td>967240</td>\n",
       "      <td>1021239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-263.0</td>\n",
       "      <td>-229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29246</th>\n",
       "      <td>5910</td>\n",
       "      <td>5438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-148.0</td>\n",
       "      <td>-148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29247</th>\n",
       "      <td>3506</td>\n",
       "      <td>4066</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29248</th>\n",
       "      <td>7491</td>\n",
       "      <td>6778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>720</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29249</th>\n",
       "      <td>7491</td>\n",
       "      <td>77875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1780</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29250 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tex_id  geotex_id  written  found  geo_id    language_text  \\\n",
       "0       12042       8388        1      1    1008            Greek   \n",
       "1       12054       8391        1      1    1008            Greek   \n",
       "2       12063       8393        1      1    1008            Greek   \n",
       "3       12064       8394        1      1    1008            Greek   \n",
       "4       17239       9507        0      1    1008            Greek   \n",
       "...       ...        ...      ...    ...     ...              ...   \n",
       "29245  967240    1021239        1      0     332            Greek   \n",
       "29246    5910       5438        1      1    1344            Greek   \n",
       "29247    3506       4066        1      1    1344  Demotic / Greek   \n",
       "29248    7491       6778        0      1     720  Demotic / Greek   \n",
       "29249    7491      77875        1      0    1780  Demotic / Greek   \n",
       "\n",
       "      material_text     y1     y2 remark  Unnamed: 10  \\\n",
       "0           papyrus  117.0  118.0    NaN          NaN   \n",
       "1           papyrus  119.0  119.0    NaN          NaN   \n",
       "2           papyrus   96.0   98.0    NaN          NaN   \n",
       "3           papyrus  131.0  131.0    NaN          NaN   \n",
       "4           papyrus  108.0  108.0    NaN          NaN   \n",
       "...             ...    ...    ...    ...          ...   \n",
       "29245       papyrus -263.0 -229.0    NaN          NaN   \n",
       "29246       papyrus -148.0 -148.0    NaN          NaN   \n",
       "29247       papyrus -150.0 -150.0    NaN          NaN   \n",
       "29248       papyrus -236.0 -236.0    new          NaN   \n",
       "29249       papyrus -236.0 -236.0    new          NaN   \n",
       "\n",
       "                                         Unnamed: 11  \n",
       "0           - y1 = earliest possible year of writing  \n",
       "1             - y2 = latest possible year of writing  \n",
       "2      [if y1 = y2, then we are certain of the date]  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "...                                              ...  \n",
       "29245                                            NaN  \n",
       "29246                                            NaN  \n",
       "29247                                            NaN  \n",
       "29248                                            NaN  \n",
       "29249                                            NaN  \n",
       "\n",
       "[29250 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- text_id: same as the text id in the features dataset\n",
    "- y1: The earliest possible date the text was written.\n",
    "- y2: The latest possible date the text was written.\n",
    "- If y1 and y2 are equal, the date of writing is known with certainty.\n",
    "'''\n",
    "\n",
    "labels_df = pd.read_csv('20240103_texts_with_dates.csv')\n",
    "\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot Encoding**\n",
    "\n",
    "Looking at the features file, it is one-hot encoded. One-hot encoding is a method used to represent categorical data as binary values. It is commonly used to convert non-numerical data into a format that algorithms can process effectively as well as avoiding implicit ordinal relationships. It works by identifying all unique categories and creating binary columns for all of them. We assign 1 to the column corresponding to the presence of that data and 0 for all other related columns.\n",
    "\n",
    "This file was not fully one-hot encoded at first. It had a 1 for all values, but a NULL for the absence of features. All NULL values should be filled with a 0 to represent the absence of that feature and clean the data more.\n",
    "\n",
    "After filling all NaN columns, we preview the first 5 columns of the features_df. We are setting the text_id as the index of the features_df because although it is a column, it is not actually a feature but rather a unique identifier for each row.\n",
    "\n",
    "Lastly, we print information about the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       text_id  nam_id_1057.0  nam_id_19683.0  nam_id_356.0  nam_id_904.0  \\\n",
      "24815    75610            0.0             0.0           0.0           0.0   \n",
      "22016    51606            0.0             0.0           0.0           0.0   \n",
      "10988    19371            0.0             0.0           0.0           0.0   \n",
      "11798    20766            0.0             0.0           0.0           0.0   \n",
      "13502    23641            0.0             0.0           0.0           0.0   \n",
      "\n",
      "       nam_id_2227.0  nam_id_726.0  nam_id_761.0  nam_id_731.0  nam_id_1246.0  \n",
      "24815            0.0           0.0           0.0           0.0            0.0  \n",
      "22016            0.0           0.0           0.0           0.0            0.0  \n",
      "10988            0.0           0.0           0.0           0.0            0.0  \n",
      "11798            0.0           0.0           0.0           0.0            0.0  \n",
      "13502            0.0           0.0           0.0           0.0            0.0  \n",
      "Number of rows: 30324\n",
      "Number of columns: 16383\n",
      "-1292.0\n",
      "1099.0\n"
     ]
    }
   ],
   "source": [
    "# Printing information about the features dataset\n",
    "features_df = features_df.fillna(0)\n",
    "\n",
    "print(features_df.sample(5).iloc[:, :10])\n",
    "\n",
    "features_df.set_index('text_id', inplace=True)\n",
    "\n",
    "\n",
    "print(f'Number of rows: {features_df.shape[0]}')\n",
    "print(f'Number of columns: {features_df.shape[1]}')\n",
    "print(labels_df['y1'].min())\n",
    "print(labels_df['y2'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Data preprocessing is the stage in machine learning where raw data is cleaned and prepared for analysis or model training. Firstly, we cleaned the data by filling NULLS with 0's for one-hot encoding. Now, we are removing duplicate rows from features_df and labels_df to ensure the dataset is clean, consistent, and free from redundancy. Duplicates can influence bias which would negatively impact the performance of our model.\n",
    "\n",
    "<font color='red'>Remove outliers?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 30324\n",
      "Number of duplicate rows: 1478\n",
      "Number of rows after removing duplicates: 28846\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the labels dataframe\n",
    "print(f\"Number of rows before removing duplicates: {features_df.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {features_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "features_df = features_df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of rows after removing duplicates: {features_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 29250\n",
      "Number of duplicate rows: 2\n",
      "Number of rows after removing duplicates: 29248\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the labels dataframe\n",
    "print(f\"Number of rows before removing duplicates: {labels_df.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {labels_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "labels_df = labels_df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of rows after removing duplicates: {labels_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are further preparing the data by removing features with low variance. Variance refers to how much the values in a feature vary. Features with very little variation don't provide value for prediction because there aren't enough differences in the data for the model to learn any patterns. By removing these low-variance features, we simplify the model, reduce overfitting, and improve computational efficiency.\n",
    "\n",
    "The threshold we chose is arbitrary since it is a hyper parameter. We tuned our models by setting various thresholds and seeing which one resulted in the lowest MAE. A threshold of 0.001 produced our best result.\n",
    "\n",
    "fit_transform calculates the variance of each feature in features_df and removes features whose variance is below our specified threshold.\n",
    "\n",
    "Printing our original and reduced shape shows how we are able to reduce our dimensions by 15,038. There were diminishing returns with feature reduction. After a certain point, it was better to have more features. Originally, we reduced to 143 columns but 1345 performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (28846, 16383)\n",
      "Reduced shape: (28846, 1345)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.001)  # Adjust threshold as needed\n",
    "features_reduced_df = selector.fit_transform(features_df)\n",
    "\n",
    "print(f\"Original shape: {features_df.shape}\")\n",
    "print(f\"Reduced shape: {features_reduced_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Singular Value Decomposition (SVD) for dimensionality reduction because it is good at reducing the number of features in high-dimensional datasets and sparse matrixes in particular. Our data was an extremely sparse matrix. SVD reduces the number of features while retaining the most important information. We chose 1000 principal components to keep because it performed well after testing various values for this hyperparameter. The explained variance ratio shows that even after reducing our features by over 15000, we still manage to capture 95% of the variance in 1000 components.\n",
    "\n",
    "<font color='red'>EXPLAIN SVD MORE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY SVD INSTEAD OF PCA?**\n",
    "\n",
    "We chose SVD over PCA for several reasons such as nonlinear relationships, sparse matrixes, and large datasets. SVD is more flexible in handling non-linear relationships and can be used as a general-purpose dimensionality reduction technique. We were able to determine our data was non-linear by comparing the results of models based on linear and nonlinear approaches. Our nonlinear models outperformed our linear models by far. SVD is also useful when dealing with sparse data, which is the case for our data. SVD seemed like a better option due to the size of our dataset as well. PCA involves calculating a covariance matrix which would take longer than SVD to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (28846, 1345)\n",
      "Reduced shape: (28846, 1000)\n",
      "[0.02962784 0.02500479 0.01815715 0.01565095 0.01279698 0.01168515\n",
      " 0.01004947 0.00987946 0.00931344 0.00879833 0.00840652 0.00839601\n",
      " 0.00809594 0.00763554 0.00718988 0.00665971 0.00635726 0.00601058\n",
      " 0.00589252 0.00575561 0.00555587 0.00538438 0.00514929 0.00505432\n",
      " 0.00497139 0.00482803 0.00466713 0.00462055 0.00449435 0.00450015\n",
      " 0.00440983 0.00434999 0.00419037 0.00417054 0.00398282 0.00395789\n",
      " 0.00388713 0.00385748 0.00381505 0.00375992 0.00364783 0.00358395\n",
      " 0.00353121 0.00346126 0.00344036 0.00339893 0.00334313 0.00329448\n",
      " 0.00327808 0.00323764 0.00320789 0.00318611 0.00315127 0.00310233\n",
      " 0.00307426 0.00303943 0.00302351 0.00295952 0.00290862 0.00287503\n",
      " 0.00286709 0.00282286 0.00280273 0.00276819 0.0027324  0.00270148\n",
      " 0.00267772 0.00264733 0.00259831 0.00260706 0.00255261 0.00252425\n",
      " 0.00250912 0.00249147 0.00248418 0.00243584 0.00241878 0.00241629\n",
      " 0.0023637  0.00236463 0.00233179 0.00229635 0.00228166 0.00225857\n",
      " 0.00224113 0.00223507 0.00221433 0.00216822 0.00216044 0.00214007\n",
      " 0.0021378  0.00212099 0.00208709 0.00207497 0.00204668 0.00202634\n",
      " 0.00200302 0.00199654 0.00198246 0.00195187 0.00194184 0.00193582\n",
      " 0.00191782 0.00191005 0.00188926 0.00188412 0.00185195 0.0018437\n",
      " 0.00182582 0.00181916 0.00180053 0.00178326 0.00177847 0.00176378\n",
      " 0.00175679 0.00174502 0.001743   0.00172772 0.00171223 0.0017037\n",
      " 0.00169173 0.00167589 0.00166211 0.00165484 0.00164778 0.00163779\n",
      " 0.00163567 0.00161588 0.00161173 0.00160585 0.00158542 0.00157443\n",
      " 0.00156666 0.00155057 0.00154559 0.00154159 0.00152186 0.00151723\n",
      " 0.00151531 0.00150442 0.00150194 0.00148275 0.00147956 0.00147411\n",
      " 0.00146562 0.00146156 0.00144689 0.00144006 0.00143127 0.00142173\n",
      " 0.00141775 0.00140446 0.00140016 0.00138989 0.00138197 0.00137071\n",
      " 0.00136443 0.00135231 0.00134061 0.00133727 0.00132714 0.00132262\n",
      " 0.00131568 0.00131186 0.00130676 0.00129947 0.00129457 0.00129199\n",
      " 0.00128523 0.00126934 0.00126305 0.00125654 0.00124983 0.00124684\n",
      " 0.00124092 0.00123485 0.00123066 0.00122139 0.00121182 0.00120593\n",
      " 0.00120524 0.00119915 0.0011903  0.001188   0.0011812  0.00117479\n",
      " 0.0011642  0.00116151 0.00115514 0.00115141 0.00114309 0.00113945\n",
      " 0.00113318 0.00112978 0.00112695 0.00111917 0.00111065 0.00110649\n",
      " 0.00110064 0.00109866 0.00109367 0.00108941 0.00108616 0.00107959\n",
      " 0.00107384 0.00107201 0.00106612 0.00106302 0.00105637 0.00104979\n",
      " 0.00104409 0.00104029 0.00103716 0.0010303  0.00102824 0.0010206\n",
      " 0.00101626 0.00101211 0.00101178 0.00100711 0.00099836 0.00099433\n",
      " 0.00098945 0.00098651 0.00098289 0.00097967 0.00097859 0.00096931\n",
      " 0.00096778 0.00096421 0.00095958 0.00095839 0.00095295 0.00095025\n",
      " 0.00094696 0.00094517 0.00094064 0.00093261 0.00093119 0.00092933\n",
      " 0.00092559 0.00092087 0.00092043 0.00091074 0.00090887 0.00090603\n",
      " 0.00090186 0.00090043 0.00089549 0.00089265 0.00089108 0.00088713\n",
      " 0.00088517 0.0008792  0.00087612 0.00087039 0.00086978 0.00086757\n",
      " 0.00086477 0.00085736 0.00085721 0.00085465 0.00085238 0.00084742\n",
      " 0.00084479 0.00083796 0.00083656 0.00083485 0.00083159 0.00082867\n",
      " 0.00082658 0.00082545 0.00082154 0.00081836 0.00081752 0.00081339\n",
      " 0.00080909 0.00080777 0.00080545 0.00079925 0.00079555 0.00079343\n",
      " 0.00078982 0.00078938 0.00078853 0.00078228 0.00078108 0.00077864\n",
      " 0.00077319 0.00077183 0.00076927 0.00076499 0.000761   0.00075921\n",
      " 0.00075564 0.00075307 0.00074955 0.00074666 0.00074549 0.00074283\n",
      " 0.00073908 0.00073728 0.00073256 0.00073199 0.00072878 0.00072685\n",
      " 0.00072405 0.00071763 0.00071669 0.00071483 0.00071161 0.00071127\n",
      " 0.00070813 0.0007063  0.00070446 0.00070292 0.00069969 0.00069532\n",
      " 0.00069378 0.0006884  0.0006857  0.00068546 0.00068178 0.00068125\n",
      " 0.00067995 0.0006772  0.00067479 0.00067264 0.00066831 0.000666\n",
      " 0.00066433 0.00066306 0.00066088 0.0006598  0.00065859 0.0006568\n",
      " 0.00065581 0.00065382 0.00064902 0.00064732 0.00064615 0.00064333\n",
      " 0.00064197 0.00063897 0.00063729 0.00063551 0.00063468 0.00063295\n",
      " 0.00063184 0.00063118 0.00062873 0.00062643 0.00062495 0.0006231\n",
      " 0.00062151 0.00062043 0.00061989 0.00061801 0.00061699 0.00061498\n",
      " 0.00061019 0.00060773 0.00060545 0.00060482 0.00060333 0.00060024\n",
      " 0.00059884 0.00059703 0.00059545 0.00059485 0.0005922  0.00059049\n",
      " 0.00059006 0.00058825 0.00058687 0.00058544 0.00058402 0.00058138\n",
      " 0.00057967 0.00057895 0.00057677 0.00057435 0.00057342 0.00057149\n",
      " 0.00057023 0.00056925 0.00056735 0.00056469 0.00056449 0.00056315\n",
      " 0.00056219 0.00055968 0.00055838 0.00055756 0.00055683 0.00055468\n",
      " 0.00055294 0.00055254 0.00055049 0.00054961 0.0005484  0.00054601\n",
      " 0.00054561 0.00054289 0.00054112 0.00054022 0.00053836 0.00053683\n",
      " 0.00053639 0.00053572 0.0005344  0.00053289 0.00053123 0.0005306\n",
      " 0.00052771 0.00052634 0.00052531 0.00052375 0.00052231 0.00052051\n",
      " 0.000519   0.00051754 0.00051607 0.00051489 0.00051435 0.00051308\n",
      " 0.00051227 0.00051077 0.00050909 0.00050792 0.00050675 0.0005062\n",
      " 0.00050476 0.00050418 0.00050228 0.00050155 0.00049852 0.00049755\n",
      " 0.00049592 0.00049541 0.00049401 0.00049378 0.00049233 0.00049015\n",
      " 0.00048921 0.00048841 0.00048745 0.00048494 0.00048501 0.00048379\n",
      " 0.00048255 0.00048149 0.0004806  0.00047851 0.00047741 0.00047629\n",
      " 0.00047556 0.00047536 0.00047318 0.000472   0.00047172 0.00046998\n",
      " 0.00046853 0.00046734 0.00046724 0.00046603 0.00046373 0.00046329\n",
      " 0.00046211 0.00046149 0.0004609  0.00045834 0.00045765 0.00045624\n",
      " 0.00045482 0.00045374 0.0004525  0.00045158 0.00045052 0.00044975\n",
      " 0.00044814 0.0004469  0.00044649 0.00044545 0.00044467 0.00044399\n",
      " 0.00044365 0.0004413  0.00043997 0.00043918 0.00043788 0.00043625\n",
      " 0.00043567 0.00043376 0.00043283 0.00043177 0.00043151 0.00043128\n",
      " 0.00042996 0.00042901 0.00042779 0.00042632 0.00042602 0.00042442\n",
      " 0.00042341 0.0004231  0.00042183 0.00042068 0.00042014 0.00041887\n",
      " 0.0004179  0.00041698 0.00041622 0.00041563 0.00041475 0.00041294\n",
      " 0.00041293 0.00041135 0.00041097 0.00040962 0.000409   0.00040826\n",
      " 0.00040681 0.00040587 0.00040547 0.00040517 0.00040359 0.00040265\n",
      " 0.00040227 0.00040145 0.00040072 0.00040033 0.00039987 0.00039794\n",
      " 0.0003976  0.00039678 0.00039568 0.00039515 0.00039348 0.00039221\n",
      " 0.00039119 0.00039053 0.0003898  0.00038944 0.00038845 0.00038768\n",
      " 0.00038712 0.0003864  0.00038533 0.0003849  0.00038326 0.00038237\n",
      " 0.00038129 0.00038065 0.00038061 0.00037865 0.00037828 0.00037761\n",
      " 0.00037731 0.00037576 0.00037547 0.00037469 0.00037392 0.00037306\n",
      " 0.00037172 0.00037119 0.00037059 0.00036873 0.00036834 0.00036774\n",
      " 0.00036743 0.00036663 0.00036525 0.000365   0.0003644  0.00036382\n",
      " 0.00036266 0.00036193 0.00036149 0.000361   0.00035984 0.00035913\n",
      " 0.00035862 0.00035782 0.00035671 0.00035591 0.00035495 0.00035463\n",
      " 0.00035409 0.00035384 0.00035236 0.00035205 0.00035157 0.0003514\n",
      " 0.00034937 0.0003488  0.00034866 0.00034796 0.00034751 0.00034658\n",
      " 0.00034599 0.00034564 0.00034483 0.00034406 0.00034319 0.00034275\n",
      " 0.00034183 0.00034103 0.00034073 0.00033951 0.00033913 0.0003386\n",
      " 0.00033828 0.00033766 0.00033678 0.00033615 0.00033498 0.00033491\n",
      " 0.00033401 0.00033361 0.00033275 0.00033239 0.00033189 0.00033113\n",
      " 0.00033077 0.00033002 0.00032857 0.00032813 0.00032752 0.00032696\n",
      " 0.00032671 0.00032638 0.00032533 0.00032447 0.00032414 0.00032373\n",
      " 0.0003228  0.00032241 0.00032204 0.00032079 0.00032071 0.00031972\n",
      " 0.00031911 0.00031856 0.00031803 0.00031724 0.00031651 0.00031596\n",
      " 0.00031556 0.00031476 0.00031432 0.00031411 0.00031347 0.00031289\n",
      " 0.00031241 0.00031153 0.00031122 0.00031084 0.00031037 0.00030962\n",
      " 0.00030917 0.00030886 0.00030803 0.00030764 0.0003063  0.00030614\n",
      " 0.00030595 0.00030538 0.00030372 0.00030335 0.0003029  0.00030278\n",
      " 0.00030219 0.00030166 0.00030078 0.00029995 0.00029943 0.00029925\n",
      " 0.00029884 0.00029837 0.00029743 0.00029719 0.0002969  0.00029617\n",
      " 0.00029528 0.00029439 0.00029385 0.00029357 0.00029317 0.00029278\n",
      " 0.00029253 0.00029159 0.00029116 0.0002909  0.00029062 0.00028964\n",
      " 0.000289   0.00028845 0.00028816 0.00028789 0.00028695 0.00028674\n",
      " 0.00028636 0.00028559 0.00028461 0.00028435 0.00028374 0.00028341\n",
      " 0.00028291 0.00028263 0.00028219 0.00028142 0.00028136 0.00028063\n",
      " 0.00028027 0.00027988 0.0002792  0.00027846 0.0002782  0.00027711\n",
      " 0.00027677 0.0002759  0.00027539 0.00027488 0.0002747  0.00027442\n",
      " 0.00027406 0.00027336 0.0002729  0.00027212 0.00027169 0.00027114\n",
      " 0.0002711  0.00027031 0.00027003 0.00026959 0.0002685  0.00026814\n",
      " 0.00026782 0.00026732 0.00026692 0.00026664 0.00026618 0.00026586\n",
      " 0.00026484 0.00026482 0.00026429 0.00026389 0.0002636  0.00026339\n",
      " 0.00026309 0.00026215 0.00026178 0.00026157 0.0002607  0.00026034\n",
      " 0.00026009 0.00025922 0.00025877 0.00025849 0.00025797 0.00025779\n",
      " 0.00025732 0.00025673 0.00025635 0.00025552 0.00025496 0.00025469\n",
      " 0.00025396 0.00025382 0.0002533  0.00025293 0.00025237 0.00025197\n",
      " 0.00025195 0.00025146 0.00025127 0.00025013 0.00025009 0.00024976\n",
      " 0.00024919 0.00024888 0.00024854 0.00024795 0.00024758 0.00024705\n",
      " 0.00024687 0.0002466  0.00024607 0.00024534 0.00024514 0.00024484\n",
      " 0.00024444 0.00024356 0.00024329 0.0002425  0.00024196 0.00024171\n",
      " 0.00024166 0.0002411  0.00024068 0.00023999 0.00023949 0.00023943\n",
      " 0.00023879 0.00023839 0.00023749 0.00023724 0.00023708 0.0002368\n",
      " 0.00023633 0.00023562 0.00023496 0.00023509 0.00023447 0.00023393\n",
      " 0.00023359 0.00023329 0.0002328  0.00023243 0.00023222 0.00023112\n",
      " 0.00023075 0.00023058 0.00023039 0.00022979 0.00022956 0.00022925\n",
      " 0.00022838 0.00022803 0.00022783 0.00022763 0.0002276  0.00022674\n",
      " 0.00022625 0.00022601 0.00022576 0.00022554 0.00022478 0.00022415\n",
      " 0.000224   0.00022392 0.00022342 0.0002233  0.00022261 0.00022229\n",
      " 0.00022169 0.00022114 0.00022065 0.00022057 0.00022005 0.00021951\n",
      " 0.00021953 0.00021848 0.00021842 0.00021811 0.00021785 0.00021723\n",
      " 0.0002167  0.00021649 0.0002163  0.0002159  0.00021531 0.00021472\n",
      " 0.00021465 0.00021425 0.00021367 0.00021343 0.00021282 0.00021259\n",
      " 0.00021234 0.00021222 0.00021185 0.00021143 0.00021044 0.00021025\n",
      " 0.00020984 0.00020972 0.00020929 0.00020922 0.00020878 0.0002082\n",
      " 0.00020802 0.00020746 0.00020721 0.00020653 0.00020634 0.00020582\n",
      " 0.00020561 0.00020523 0.00020449 0.000204   0.00020378 0.00020344\n",
      " 0.00020337 0.00020316 0.00020295 0.00020217 0.00020203 0.00020152\n",
      " 0.00020094 0.00020067 0.00019979 0.00019986 0.00019948 0.00019936\n",
      " 0.00019896 0.0001984  0.00019824 0.00019814 0.00019792 0.00019715\n",
      " 0.000197   0.00019654 0.00019607 0.00019553 0.00019471 0.00019486\n",
      " 0.00019419 0.00019404 0.0001936  0.00019286 0.00019257 0.00019231\n",
      " 0.00019198 0.00019193 0.00019135 0.00019123 0.00019106 0.00019061\n",
      " 0.00019029 0.00018998 0.00018986 0.00018946 0.00018898 0.00018879\n",
      " 0.000188   0.00018763 0.00018699 0.0001864  0.00018583 0.00018559\n",
      " 0.00018549 0.00018477 0.00018478 0.00018437 0.00018399 0.00018395\n",
      " 0.00018362 0.00018331 0.00018283 0.00018256 0.00018241 0.00018189\n",
      " 0.00018181 0.0001813  0.00018072 0.0001801  0.00017934 0.00017936\n",
      " 0.00017876 0.00017832 0.00017819 0.00017784 0.00017712 0.00017697\n",
      " 0.0001765  0.00017559 0.00017527 0.00017495 0.00017449 0.00017423\n",
      " 0.00017426 0.00017368 0.00017281 0.00017223 0.00017161 0.00017146\n",
      " 0.00017088 0.00017093 0.00016981 0.00016909 0.00016936 0.00016881\n",
      " 0.00016812 0.00016749 0.00016754 0.00016678 0.00016636 0.00016634\n",
      " 0.00016614 0.00016551 0.00016519 0.00016427 0.00016383 0.00016269\n",
      " 0.00016261 0.0001622  0.00016202 0.00016076]\n",
      "0.9536659040503459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)\n",
    "# svd = TruncatedSVD(n_components=2000, random_state=42)\n",
    "features_svd = svd.fit_transform(features_reduced_df)\n",
    "\n",
    "print(f\"Original shape: {features_reduced_df.shape}\")\n",
    "print(f\"Reduced shape: {features_svd.shape}\")\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(sum(svd.explained_variance_ratio_))  # Total variance retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell converts our features_svd array back into a dataframe features_svd_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming reduced_features is your NumPy array from SVD or Variance Thresholding\n",
    "# features_df['text_id'] contains the IDs you need to reattach\n",
    "features_df.reset_index(inplace=True)\n",
    "features_svd_df = pd.DataFrame(features_svd, columns=[f'feature_{i}' for i in range(features_svd.shape[1])])\n",
    "features_svd_df['text_id'] = features_df['text_id'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important cells because it prepares our training set. Our original label and features files have around 20k and 30k rows respectively. Not every row of data in the features dataset has a corresponding row of data in the labels dataset and vice versa. In order to train our model, we need to give it data that it already knows the answers for - the ground truths. This is the metric we will compare our predictions against to guage the performance of our models. Although we cleaned our data for duplicates before, we wanted to ensure the data was thoroughly prepared as we inch closer to training our model.\n",
    "\n",
    "After removing duplicates and data that didn't appear in both the label and features datasets, we were left with 8565 datapoints with corresponding labels. This is the data we use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text IDs missing in labels: 20281\n",
      "Text IDs missing in features: 12381\n",
      "Updated features shape: (8565, 1001)\n",
      "Updated labels shape: (13013, 12)\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 0\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 4448\n",
      "Aligned features shape: (8565, 1001)\n",
      "Aligned labels shape: (8565, 12)\n"
     ]
    }
   ],
   "source": [
    "# Get text_ids in each dataset\n",
    "features_text_ids = set(features_svd_df['text_id'])\n",
    "labels_text_ids = set(labels_df['tex_id'])\n",
    "\n",
    "# Find unmatched text_ids\n",
    "missing_in_labels = features_text_ids - labels_text_ids\n",
    "missing_in_features = labels_text_ids - features_text_ids\n",
    "\n",
    "print(f\"Text IDs missing in labels: {len(missing_in_labels)}\")\n",
    "print(f\"Text IDs missing in features: {len(missing_in_features)}\")\n",
    "\n",
    "# Keep only common text_ids\n",
    "common_text_ids = features_text_ids & labels_text_ids\n",
    "\n",
    "# Filter features and labels datasets\n",
    "features_common_df = features_svd_df[features_svd_df['text_id'].isin(common_text_ids)]\n",
    "labels_common_df = labels_df[labels_df['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "# Check the updated shapes\n",
    "print(f\"Updated features shape: {features_common_df.shape}\")\n",
    "print(f\"Updated labels shape: {labels_common_df.shape}\")\n",
    "\n",
    "# Check for duplicated rows\n",
    "duplicate_rows = features_common_df.duplicated()\n",
    "\n",
    "# Check for duplicated text_ids\n",
    "duplicate_text_ids = features_common_df['text_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_text_ids.sum()}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_label_rows = labels_common_df.duplicated()\n",
    "\n",
    "# Check for duplicate text_ids\n",
    "duplicate_label_text_ids = labels_common_df['tex_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_label_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_label_text_ids.sum()}\")\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "features = features_common_df.drop_duplicates(subset='text_id', keep='first')\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "labels = labels_common_df.drop_duplicates(subset='tex_id', keep='first')\n",
    "\n",
    "\n",
    "# Ensure alignment of text_ids between features and labels\n",
    "common_text_ids = set(features['text_id']) & set(labels['tex_id'])\n",
    "\n",
    "# Filter again if necessary\n",
    "features_filtered_df = features[features['text_id'].isin(common_text_ids)]\n",
    "labels_filtered_df = labels[labels['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "print(f\"Aligned features shape: {features_filtered_df.shape}\")\n",
    "print(f\"Aligned labels shape: {labels_filtered_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the index of our dataframes to the text_ids since are not considered features, but rather unique identifiers of each datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filtered_df.set_index('text_id', inplace=True)\n",
    "\n",
    "labels_filtered_df.rename(columns={'tex_id': 'text_id'}, inplace=True)\n",
    "labels_filtered_df.set_index('text_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our task involves predicting the year a text was written, retaining the other ground truth labels was unnecessary. Removing them avoids confusion and ensures our process is focused solely on the data relevant to our task.\n",
    "\n",
    "<font color=\"red\"> Should we convert the other labels into features before dimension reduction?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12042</th>\n",
       "      <td>117.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12054</th>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12064</th>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17239</th>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703104</th>\n",
       "      <td>-250.0</td>\n",
       "      <td>-175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703317</th>\n",
       "      <td>-263.0</td>\n",
       "      <td>-229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>-148.0</td>\n",
       "      <td>-148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>-150.0</td>\n",
       "      <td>-150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>-236.0</td>\n",
       "      <td>-236.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y1     y2\n",
       "text_id              \n",
       "12042    117.0  118.0\n",
       "12054    119.0  119.0\n",
       "12063     96.0   98.0\n",
       "12064    131.0  131.0\n",
       "17239    108.0  108.0\n",
       "...        ...    ...\n",
       "703104  -250.0 -175.0\n",
       "703317  -263.0 -229.0\n",
       "5910    -148.0 -148.0\n",
       "3506    -150.0 -150.0\n",
       "7491    -236.0 -236.0\n",
       "\n",
       "[8565 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_final_df = labels_filtered_df[['y1','y2']]\n",
    "labels_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this step is redundant, we wanted to ensure all of our datapoints were merged with a corresponding label via an inner join. This could be skipped altogether since we are separating the y label from the X features when training our model, but having it as one dataframe in the beginning aligned with our approaches throughout the course.\n",
    "\n",
    "<font color=\"red\">Can we replace the cell above where we filter for records in both features_df and labels_df by only using this method?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8565, 1002)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge on text_id\n",
    "merged_df = features_filtered_df.merge(labels_final_df, on='text_id', how='inner')\n",
    "\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification or Regression?</h1>\n",
    "\n",
    "Here we are creating two variables that indicate how many rows exist where y1=y2 and y1!=y2 and displaying those results.\n",
    "\n",
    "Since we have more rows where an exact year is known (5,239) we believe regression was the better approach because predicting an exact year (a continuous variable) aligns with the strengths of regression.\n",
    "\n",
    "If we had more cases where y1!=y2, then there would be more uncertainty about the exact date of our texts, justifying the use of a classifcation model where we simplify the task by binning the ranges into predifined categories (100-199AD, 200-299AD).\n",
    "\n",
    "Based on the context of our training data, we believed regression was the better approach because the majority of our ground truths were a single continous year vs a range of years.\n",
    "\n",
    "<b>We chose Regression</b>\n",
    "\n",
    "<font color=\"red\"> Should we create classification models anyway for comparison?</font>\n",
    "\n",
    "<font color=\"red\">Should we combine classifcation and regression models into one model?\n",
    "ex. classifcation followed by regression\n",
    "ex. regression followed by classification\n",
    "ex. multi-output neural network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where y1 = y2: 5239\n",
      "Number of rows where y1 != y2: 3326\n"
     ]
    }
   ],
   "source": [
    "equal_rows = merged_df[merged_df['y1'] == merged_df['y2']].shape[0]\n",
    "unequal_rows = merged_df[merged_df['y1'] != merged_df['y2']].shape[0]\n",
    "\n",
    "print(f\"Number of rows where y1 = y2: {equal_rows}\")\n",
    "print(f\"Number of rows where y1 != y2: {unequal_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we chose regression, we created a target column that handles both cases when y1 equals y2 (continuous) and when y1 didn't equal y2 (range of years).\n",
    "\n",
    "y_target simply became y1 if they were equal\n",
    "y_target took the midpoint of y1 and y2 if they weren't equal. The midpoint serves as a reasonable estimate for a range when an exact year isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_993</th>\n",
       "      <th>feature_994</th>\n",
       "      <th>feature_995</th>\n",
       "      <th>feature_996</th>\n",
       "      <th>feature_997</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_999</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404936</td>\n",
       "      <td>-0.332399</td>\n",
       "      <td>0.569584</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>0.118471</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.153942</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.075361</td>\n",
       "      <td>-0.012450</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>-0.014151</td>\n",
       "      <td>-0.024547</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.194844</td>\n",
       "      <td>-0.455850</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>-0.084557</td>\n",
       "      <td>0.912922</td>\n",
       "      <td>0.938616</td>\n",
       "      <td>0.366342</td>\n",
       "      <td>-0.087693</td>\n",
       "      <td>-0.983342</td>\n",
       "      <td>-0.075471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.030107</td>\n",
       "      <td>-0.024161</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669130</td>\n",
       "      <td>-0.418529</td>\n",
       "      <td>0.354021</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.230967</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>-0.131145</td>\n",
       "      <td>-0.388602</td>\n",
       "      <td>0.684473</td>\n",
       "      <td>0.321883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>-0.016734</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>-109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.226901</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>0.716642</td>\n",
       "      <td>-0.143475</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>0.681385</td>\n",
       "      <td>0.182460</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>-0.469498</td>\n",
       "      <td>-0.267133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035404</td>\n",
       "      <td>-0.097845</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403593</td>\n",
       "      <td>-0.306048</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.177825</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>-0.021976</td>\n",
       "      <td>0.124563</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>0.217894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981643</th>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.044002</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>548.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981644</th>\n",
       "      <td>0.222981</td>\n",
       "      <td>0.390343</td>\n",
       "      <td>0.176930</td>\n",
       "      <td>-0.095436</td>\n",
       "      <td>-0.083316</td>\n",
       "      <td>0.120702</td>\n",
       "      <td>0.464449</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.487788</td>\n",
       "      <td>-0.111622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>553.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981646</th>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>549.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981648</th>\n",
       "      <td>0.072397</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.118935</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>-0.261060</td>\n",
       "      <td>0.207830</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>500.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>549.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981666</th>\n",
       "      <td>0.325492</td>\n",
       "      <td>-0.261670</td>\n",
       "      <td>0.346592</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.076177</td>\n",
       "      <td>0.163047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows Ã— 1003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "text_id                                                                     \n",
       "1         0.404936  -0.332399   0.569584   0.244467   0.118471  -0.012269   \n",
       "2         1.194844  -0.455850   0.790932  -0.084557   0.912922   0.938616   \n",
       "3         0.669130  -0.418529   0.354021   0.003984   0.230967   0.093175   \n",
       "4         1.226901  -0.607306   0.716642  -0.143475   0.932176   0.681385   \n",
       "5         0.403593  -0.306048   0.475889   0.177825   0.139036  -0.021976   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "981643    0.008250   0.016243   0.018408   0.012553  -0.044002   0.041111   \n",
       "981644    0.222981   0.390343   0.176930  -0.095436  -0.083316   0.120702   \n",
       "981646    0.019691   0.037447   0.039414   0.022553  -0.083950   0.074489   \n",
       "981648    0.072397   0.109943   0.118935   0.083189  -0.261060   0.207830   \n",
       "981666    0.325492  -0.261670   0.346592   0.218479  -0.000641  -0.137728   \n",
       "\n",
       "         feature_6  feature_7  feature_8  feature_9  ...  feature_993  \\\n",
       "text_id                                              ...                \n",
       "1         0.153942   0.019276   0.003331   0.233105  ...     0.006794   \n",
       "2         0.366342  -0.087693  -0.983342  -0.075471  ...     0.029750   \n",
       "3        -0.131145  -0.388602   0.684473   0.321883  ...     0.022786   \n",
       "4         0.182460   0.103260  -0.469498  -0.267133  ...    -0.035404   \n",
       "5         0.124563   0.050058  -0.034562   0.217894  ...     0.002199   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "981643    0.018783   0.001615   0.011903   0.007014  ...    -0.002129   \n",
       "981644    0.464449   0.153280   0.487788  -0.111622  ...    -0.000316   \n",
       "981646    0.025522   0.007548   0.021971   0.009494  ...     0.000879   \n",
       "981648    0.096346   0.025396   0.049978   0.062749  ...    -0.005298   \n",
       "981666    0.036744   0.155500   0.076177   0.163047  ...     0.014535   \n",
       "\n",
       "         feature_994  feature_995  feature_996  feature_997  feature_998  \\\n",
       "text_id                                                                    \n",
       "1           0.075361    -0.012450     0.011055     0.047048    -0.014151   \n",
       "2          -0.032232     0.014291    -0.030107    -0.024161    -0.011760   \n",
       "3          -0.009252     0.038818    -0.002101     0.028129    -0.016734   \n",
       "4          -0.097845     0.001688     0.080584     0.043998     0.040462   \n",
       "5          -0.002162     0.007168     0.001697    -0.003452    -0.001445   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "981643     -0.002368     0.003550     0.002539     0.004470    -0.000576   \n",
       "981644      0.002716     0.007476     0.001311     0.000043     0.009732   \n",
       "981646      0.002465     0.000985     0.000319     0.000345    -0.001285   \n",
       "981648      0.002257     0.007938    -0.000986     0.003599     0.002233   \n",
       "981666      0.005975    -0.001595     0.008034    -0.005991     0.010769   \n",
       "\n",
       "         feature_999     y1     y2  y_target  \n",
       "text_id                                       \n",
       "1          -0.024547 -124.0 -124.0    -124.0  \n",
       "2           0.020541 -112.0 -112.0    -112.0  \n",
       "3           0.019266 -109.0 -109.0    -109.0  \n",
       "4           0.092145 -108.0 -108.0    -108.0  \n",
       "5           0.011756 -106.0 -106.0    -106.0  \n",
       "...              ...    ...    ...       ...  \n",
       "981643      0.000890  548.0  548.0     548.0  \n",
       "981644     -0.003175  553.0  553.0     553.0  \n",
       "981646      0.004310  549.0  549.0     549.0  \n",
       "981648      0.001810  500.0  599.0     549.5  \n",
       "981666     -0.002167  133.0  133.0     133.0  \n",
       "\n",
       "[8565 rows x 1003 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['y_target'] = merged_df.apply(lambda row: row['y1'] if row['y1'] == row['y2'] else (row['y1'] + row['y2']) / 2, axis=1)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our new ground truth y_target, we no longer needed the labels y1 and y2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_991</th>\n",
       "      <th>feature_992</th>\n",
       "      <th>feature_993</th>\n",
       "      <th>feature_994</th>\n",
       "      <th>feature_995</th>\n",
       "      <th>feature_996</th>\n",
       "      <th>feature_997</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_999</th>\n",
       "      <th>y_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404936</td>\n",
       "      <td>-0.332399</td>\n",
       "      <td>0.569584</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>0.118471</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.153942</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036339</td>\n",
       "      <td>0.028035</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.075361</td>\n",
       "      <td>-0.012450</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>-0.014151</td>\n",
       "      <td>-0.024547</td>\n",
       "      <td>-124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.194844</td>\n",
       "      <td>-0.455850</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>-0.084557</td>\n",
       "      <td>0.912922</td>\n",
       "      <td>0.938616</td>\n",
       "      <td>0.366342</td>\n",
       "      <td>-0.087693</td>\n",
       "      <td>-0.983342</td>\n",
       "      <td>-0.075471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025149</td>\n",
       "      <td>-0.023396</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.030107</td>\n",
       "      <td>-0.024161</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>-112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669130</td>\n",
       "      <td>-0.418529</td>\n",
       "      <td>0.354021</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.230967</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>-0.131145</td>\n",
       "      <td>-0.388602</td>\n",
       "      <td>0.684473</td>\n",
       "      <td>0.321883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>-0.016734</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>-109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.226901</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>0.716642</td>\n",
       "      <td>-0.143475</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>0.681385</td>\n",
       "      <td>0.182460</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>-0.469498</td>\n",
       "      <td>-0.267133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>-0.035404</td>\n",
       "      <td>-0.097845</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403593</td>\n",
       "      <td>-0.306048</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.177825</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>-0.021976</td>\n",
       "      <td>0.124563</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>0.217894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>-0.008616</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>-106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981643</th>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.044002</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981644</th>\n",
       "      <td>0.222981</td>\n",
       "      <td>0.390343</td>\n",
       "      <td>0.176930</td>\n",
       "      <td>-0.095436</td>\n",
       "      <td>-0.083316</td>\n",
       "      <td>0.120702</td>\n",
       "      <td>0.464449</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.487788</td>\n",
       "      <td>-0.111622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>-0.002140</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981646</th>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981648</th>\n",
       "      <td>0.072397</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.118935</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>-0.261060</td>\n",
       "      <td>0.207830</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>549.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981666</th>\n",
       "      <td>0.325492</td>\n",
       "      <td>-0.261670</td>\n",
       "      <td>0.346592</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.076177</td>\n",
       "      <td>0.163047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows Ã— 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "text_id                                                                     \n",
       "1         0.404936  -0.332399   0.569584   0.244467   0.118471  -0.012269   \n",
       "2         1.194844  -0.455850   0.790932  -0.084557   0.912922   0.938616   \n",
       "3         0.669130  -0.418529   0.354021   0.003984   0.230967   0.093175   \n",
       "4         1.226901  -0.607306   0.716642  -0.143475   0.932176   0.681385   \n",
       "5         0.403593  -0.306048   0.475889   0.177825   0.139036  -0.021976   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "981643    0.008250   0.016243   0.018408   0.012553  -0.044002   0.041111   \n",
       "981644    0.222981   0.390343   0.176930  -0.095436  -0.083316   0.120702   \n",
       "981646    0.019691   0.037447   0.039414   0.022553  -0.083950   0.074489   \n",
       "981648    0.072397   0.109943   0.118935   0.083189  -0.261060   0.207830   \n",
       "981666    0.325492  -0.261670   0.346592   0.218479  -0.000641  -0.137728   \n",
       "\n",
       "         feature_6  feature_7  feature_8  feature_9  ...  feature_991  \\\n",
       "text_id                                              ...                \n",
       "1         0.153942   0.019276   0.003331   0.233105  ...    -0.036339   \n",
       "2         0.366342  -0.087693  -0.983342  -0.075471  ...     0.025149   \n",
       "3        -0.131145  -0.388602   0.684473   0.321883  ...     0.015310   \n",
       "4         0.182460   0.103260  -0.469498  -0.267133  ...     0.063221   \n",
       "5         0.124563   0.050058  -0.034562   0.217894  ...     0.005871   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "981643    0.018783   0.001615   0.011903   0.007014  ...     0.005598   \n",
       "981644    0.464449   0.153280   0.487788  -0.111622  ...    -0.002949   \n",
       "981646    0.025522   0.007548   0.021971   0.009494  ...     0.001661   \n",
       "981648    0.096346   0.025396   0.049978   0.062749  ...    -0.001269   \n",
       "981666    0.036744   0.155500   0.076177   0.163047  ...     0.005230   \n",
       "\n",
       "         feature_992  feature_993  feature_994  feature_995  feature_996  \\\n",
       "text_id                                                                    \n",
       "1           0.028035     0.006794     0.075361    -0.012450     0.011055   \n",
       "2          -0.023396     0.029750    -0.032232     0.014291    -0.030107   \n",
       "3           0.003670     0.022786    -0.009252     0.038818    -0.002101   \n",
       "4           0.044466    -0.035404    -0.097845     0.001688     0.080584   \n",
       "5          -0.008616     0.002199    -0.002162     0.007168     0.001697   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "981643     -0.002627    -0.002129    -0.002368     0.003550     0.002539   \n",
       "981644     -0.002140    -0.000316     0.002716     0.007476     0.001311   \n",
       "981646      0.002340     0.000879     0.002465     0.000985     0.000319   \n",
       "981648      0.001269    -0.005298     0.002257     0.007938    -0.000986   \n",
       "981666     -0.001312     0.014535     0.005975    -0.001595     0.008034   \n",
       "\n",
       "         feature_997  feature_998  feature_999  y_target  \n",
       "text_id                                                   \n",
       "1           0.047048    -0.014151    -0.024547    -124.0  \n",
       "2          -0.024161    -0.011760     0.020541    -112.0  \n",
       "3           0.028129    -0.016734     0.019266    -109.0  \n",
       "4           0.043998     0.040462     0.092145    -108.0  \n",
       "5          -0.003452    -0.001445     0.011756    -106.0  \n",
       "...              ...          ...          ...       ...  \n",
       "981643      0.004470    -0.000576     0.000890     548.0  \n",
       "981644      0.000043     0.009732    -0.003175     553.0  \n",
       "981646      0.000345    -0.001285     0.004310     549.0  \n",
       "981648      0.003599     0.002233     0.001810     549.5  \n",
       "981666     -0.005991     0.010769    -0.002167     133.0  \n",
       "\n",
       "[8565 rows x 1001 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.drop(columns=['y1','y2'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TRAINING & COMPARING OUR MODELS</h1>\n",
    "\n",
    "This code splits our dataset into training and testing subsets to prepare it for our machine learning algorithms.\n",
    "\n",
    "X contains all of our features\n",
    "y contains our label\n",
    "\n",
    "We are then splitting the data into training and testing subsets, using 20% of the data for testing and 80% for training. We set random_sate =42 for reproducibility.\n",
    "\n",
    "The purpose of splitting our data is to evaluate how well the model generalizes to unseen data. Our training sets are used to train the model while the testing set is used afterward to assess the model's performance on data it hasn't seen during training.\n",
    "\n",
    "We did not scale our data because the original data was one-hot encoded and reducced using Variance Thresholding and SVD. Variance Thresholding only removes features with low variance and doesn't change their scales. SVD produces principle components that are linear combinations of our original features, but SVD already scales them to optimize variance. The components were normalized internally in the decomposition process of the SVD class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])\n",
    "y = merged_df['y_target']\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LINEAR OR NONLINEAR RELATIONSHIPS?</h1>\n",
    "After training all of our models, it is apparent our the relationships in our data may be nonlinear.\n",
    "\n",
    "Our Lasso and Ridge Regression lienar models performed significantly worse. These models assume that the relationship between the features and target is linear.\n",
    "\n",
    "Our RandomForest and XGBoost ensemble nonlinear models performed far better. These models capture nonlinear relationships in the data. They work by splitting the features into smaller tress and makes decisions based on those trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LASSO</h1>\n",
    "\n",
    "We are using Lasso  linear regression model with cross validation, using 5 folds. Lasso adds an L1 regularization term to the update function. Lasso is able to shrink some coefficients to 0, basically reducing some of our features. Lasso was our poorest performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso - Best alpha: 0.10914421422151917\n",
      "Train MAE: 113.21444925516678\n",
      "Test MAE: 125.15683946474095\n",
      "Selected features by Lasso:\n",
      "feature_0       19.243864\n",
      "feature_1      285.816029\n",
      "feature_2       21.054207\n",
      "feature_3       53.341782\n",
      "feature_4     -262.346585\n",
      "                  ...    \n",
      "feature_988      0.911295\n",
      "feature_990     33.784021\n",
      "feature_995    -99.722730\n",
      "feature_996    -16.106377\n",
      "feature_999    -44.783744\n",
      "Length: 565, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Lasso with cross-validation\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Lasso - Best alpha:\", lasso.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, lasso.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, lasso.predict(X_test)))\n",
    "\n",
    "lasso_features = pd.Series(lasso.coef_, index=X.columns)\n",
    "print(\"Selected features by Lasso:\")\n",
    "print(lasso_features[lasso_features != 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RIDGE</h1>\n",
    "\n",
    "The code below performs ridge regression with cross-validation. Ridge regression adds an L2 regularization to the linear regression model, which penalizes large coefficient values and helps prevent overfitting. The strength of the regularization is controlled by the alpha parameter when instantiating the RidgeCV class. We are also performing 5-fold cross validation, which means it splits the training data into 5 parts, trains the model of 4 parts and validates on the remaing part, and repeats this process 5 times as it iterates through each fold.\n",
    "\n",
    "The mean absolute error, MAE, measures the average absolute difference between our predicted values and the ground truths. The lower the MAE, the better. After printing the MAE of our best Ridge model's best parameters, we had a Train and TEST MAE of 109.62 and 124.12. These are some of our lowest scores after comparing all of our models. The poor results on Ridge and Lasso compared to our other models suggests that our data is nonlinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge - Best alpha: 10.0\n",
      "Train MAE: 109.61849945515857\n",
      "Test MAE: 124.1208769261426\n",
      "Ridge coefficients:\n",
      "feature_1      282.559384\n",
      "feature_35     154.172779\n",
      "feature_459    139.881412\n",
      "feature_139    136.531244\n",
      "feature_458    113.989942\n",
      "feature_711    102.902482\n",
      "feature_635    100.589513\n",
      "feature_36      97.584438\n",
      "feature_713     97.335522\n",
      "feature_578     94.693208\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ridge with cross-validation\n",
    "ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Ridge - Best alpha:\", ridge.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, ridge.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, ridge.predict(X_test)))\n",
    "\n",
    "ridge_features = pd.Series(ridge.coef_, index=X.columns)\n",
    "print(\"Ridge coefficients:\")\n",
    "print(ridge_features.sort_values(ascending=False).head(10))  # Top 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RANDOM FOREST</h1>\n",
    "\n",
    "This code implements a RandomForest model designed for regression tasks. It builds 100 decision tress in the \"forest\" (model).\n",
    "\n",
    "A random forest is an ensemble learning algorithm that combines multiple decision tress to improve prediction accuracy and reduce overfitting. The algorithm does something called bootstrap sampling, which randomly selects samples with replacement from the training data to create several datasets for training the individual trees we specified in the parameter. For each decision tree, the algorithm selects a random subset of features at each split. Each tree in the forest predicts a value and the final prediction is the average of all tree predictions in the forest. Random Forest is effective at solving tasks because it uses many tress instead of a single deicision tree to make an accurate prediction. This ensemble reduces variance in the predictions. It also handles complex, nonlinear relationships in the data which seems to be the case in our dataset.\n",
    "\n",
    "After 8m 15s of execution, our initial RandomForest produced results far better than our Linear and Ridge models, having a training MAE of 30.7 and test MAE of 78.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 30.740977718639943\n",
      "Random Forest - Test MAE: 78.56731082965455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OPTIMIZING RANDOM FOREST</h1>\n",
    "\n",
    "The RandomForest regressor was one of our highest performing models. It performed far better than Lasso and Ridge, so we wanted to improve the model by searching for better hyper parameters.\n",
    "\n",
    "We decided to use GridSearch cross validation with performs a systematic search over a range of hyperparameter combinations we assign for our RandomForestRegressor model. We are passing in parameters to control the amount of trees in the forest, the maximum depth of each tree, and the minimum number of samples required to split a node in each tree. We are also using 5 fold cross validation.\n",
    "\n",
    "After execution, we found our best parameters were:\n",
    "\n",
    "Best Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 300}\n",
    "\n",
    "Best Random Forest Test MAE: 92.32550495382549\n",
    "\n",
    "\n",
    "BEWARE!! RUNNING THIS CELL TAKES 3 HOURS TO EXECUTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m300\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m30\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmin_samples_split\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(RandomForestRegressor(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m), param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Parameters:\u001b[39m\u001b[39m\"\u001b[39m, grid_search\u001b[39m.\u001b[39mbest_params_)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Random Forest Test MAE:\u001b[39m\u001b[39m\"\u001b[39m, mean_absolute_error(y_test, grid_search\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mpredict(X_test)))\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m   1021\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    966\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m         clone(base_estimator),\n\u001b[1;32m    968\u001b[0m         X,\n\u001b[1;32m    969\u001b[0m         y,\n\u001b[1;32m    970\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    971\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    972\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    973\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    974\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    975\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    976\u001b[0m     )\n\u001b[1;32m    977\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    978\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params),\n\u001b[1;32m    979\u001b[0m         \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49msplitter\u001b[39m.\u001b[39;49msplit)),\n\u001b[1;32m    980\u001b[0m     )\n\u001b[1;32m    981\u001b[0m )\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    890\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    490\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    491\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    492\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    493\u001b[0m )(\n\u001b[1;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    495\u001b[0m         t,\n\u001b[1;32m    496\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    497\u001b[0m         X,\n\u001b[1;32m    498\u001b[0m         y,\n\u001b[1;32m    499\u001b[0m         sample_weight,\n\u001b[1;32m    500\u001b[0m         i,\n\u001b[1;32m    501\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    502\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    503\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    504\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[39m=\u001b[39;49mmissing_values_in_feature_mask,\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     tree\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    193\u001b[0m         X,\n\u001b[1;32m    194\u001b[0m         y,\n\u001b[1;32m    195\u001b[0m         sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight,\n\u001b[1;32m    196\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[39m=\u001b[39;49mmissing_values_in_feature_mask,\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[39m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[39m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/PycharmProjects/machinelearning/machine-learning-final/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Random Forest Test MAE:\", mean_absolute_error(y_test, grid_search.best_estimator_.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding our best hyper parameters, we reran the RandomForestRegressor with the new hyper parameters to see our results. Expectedly, they improved. However, we wanted to explore other models to see if there was even better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 31.50111052149672\n",
      "Random Forest - Test MAE: 77.96461167137234\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(max_depth=20, min_samples_split=5, n_estimators=300, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBOOST (EXTREME GRADIENT BOOSTING)</h1>\n",
    "\n",
    "This model is a gradient boosting ensemble learning algorithm. XGBoost is an efficient and accurate gradient boosintg framework for machine learning. We experimented with a bunch of different parameters manually, slowly improving our model from 81 MAE down to 71 MAE.\n",
    "\n",
    "Boosting is an ensemble learning technique that combines weak learners (like shallow decision trees) sequentially to form a strong learner. Each new tree is trained to correct errors made by the previous shallow trees. Gradient boosting minimizes the update function by computing gradients of the loss with respect to the model's predictions. The XGBoost frameowkr in particular has regularization to prevent overfitting, is optimized for speed and low memory usage, and supports sparse data.\n",
    "\n",
    "Only taking 20 seconds to run, our XGBoost model was our most successful and most efficient compared to the 18 minutes it took our RandomForest model. We were able to adjust our model to achieve an MAE of 71.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 71.1634089561213\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.7, learning_rate = 0.031,\n",
    "                          max_depth = 7, alpha = 25, n_estimators = 350)\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R^2 value for our XGBoost model was 0.84 which indicates that 84% of the variation in our target variable was captured by the model. This indicates that the features we reduced to are highly predictive of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 71.1634089561213\n",
      "XGBoost R-squared: 0.8406828639172944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate MAE and R-squared\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n",
    "print(f\"XGBoost R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])  # Replace with your feature dataframe\n",
    "y = merged_df['y_target']  # Replace with your target column\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightgbm\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlgb\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#Y130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_absolute_error\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adriangallant/PycharmProjects/machinelearning/machine-learning-final/ancient_text_dater.ipynb#Y130sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Initialize LightGBM Regressor\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize LightGBM Regressor\n",
    "lgb_reg = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=300,\n",
    "    max_depth=-1,  # No max depth\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_lgb = lgb_reg.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "lgb_mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "print(f\"LightGBM MAE: {lgb_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM MAE: 226.18691305100145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create a pipeline to scale the data and apply SVR\n",
    "svm_pipeline = make_pipeline(\n",
    "    StandardScaler(),  # SVM benefits from scaling the data\n",
    "    SVR(kernel='rbf', C=1.0, epsilon=0.1)  # Radial Basis Function kernel\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "svm_mae = mean_absolute_error(y_test, y_pred_svm)\n",
    "print(f\"SVM MAE: {svm_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_0': 64961.3671875,\n",
       " 'feature_1': 2809113.25,\n",
       " 'feature_2': 1221373.875,\n",
       " 'feature_3': 651875.5,\n",
       " 'feature_4': 2423238.5,\n",
       " 'feature_5': 1188979.375,\n",
       " 'feature_6': 278822.375,\n",
       " 'feature_7': 257873.984375,\n",
       " 'feature_8': 483481.8125,\n",
       " 'feature_9': 169527.734375,\n",
       " 'feature_10': 343841.78125,\n",
       " 'feature_11': 237333.296875,\n",
       " 'feature_12': 132837.421875,\n",
       " 'feature_13': 678613.375,\n",
       " 'feature_14': 392072.40625,\n",
       " 'feature_15': 260833.96875,\n",
       " 'feature_16': 524084.4375,\n",
       " 'feature_17': 503391.5625,\n",
       " 'feature_18': 371862.78125,\n",
       " 'feature_19': 591305.5,\n",
       " 'feature_20': 109453.4921875,\n",
       " 'feature_21': 128313.0625,\n",
       " 'feature_22': 329286.125,\n",
       " 'feature_23': 153892.5625,\n",
       " 'feature_24': 117306.0,\n",
       " 'feature_25': 268342.5625,\n",
       " 'feature_26': 189703.0625,\n",
       " 'feature_27': 242569.0,\n",
       " 'feature_28': 161073.734375,\n",
       " 'feature_29': 207829.578125,\n",
       " 'feature_30': 95094.0078125,\n",
       " 'feature_31': 675708.375,\n",
       " 'feature_32': 186232.203125,\n",
       " 'feature_33': 321032.0,\n",
       " 'feature_34': 697289.0625,\n",
       " 'feature_35': 565854.125,\n",
       " 'feature_36': 587619.25,\n",
       " 'feature_37': 185196.34375,\n",
       " 'feature_38': 70839.484375,\n",
       " 'feature_39': 245195.765625,\n",
       " 'feature_40': 116885.2265625,\n",
       " 'feature_41': 154950.640625,\n",
       " 'feature_42': 124159.0,\n",
       " 'feature_43': 51973.10546875,\n",
       " 'feature_44': 100156.1015625,\n",
       " 'feature_45': 53682.83203125,\n",
       " 'feature_46': 159366.546875,\n",
       " 'feature_47': 156084.375,\n",
       " 'feature_48': 76777.6015625,\n",
       " 'feature_49': 67387.59375,\n",
       " 'feature_50': 49052.3671875,\n",
       " 'feature_51': 38589.6015625,\n",
       " 'feature_52': 66243.5546875,\n",
       " 'feature_53': 46958.33203125,\n",
       " 'feature_54': 215932.8125,\n",
       " 'feature_55': 53628.82421875,\n",
       " 'feature_56': 76119.7890625,\n",
       " 'feature_57': 109536.078125,\n",
       " 'feature_58': 42638.71875,\n",
       " 'feature_59': 83881.5859375,\n",
       " 'feature_60': 23514.669921875,\n",
       " 'feature_61': 233610.203125,\n",
       " 'feature_62': 66615.046875,\n",
       " 'feature_63': 46608.88671875,\n",
       " 'feature_64': 53848.1015625,\n",
       " 'feature_65': 55064.0625,\n",
       " 'feature_66': 90311.796875,\n",
       " 'feature_67': 65419.49609375,\n",
       " 'feature_68': 39592.69921875,\n",
       " 'feature_69': 31341.736328125,\n",
       " 'feature_70': 182820.578125,\n",
       " 'feature_71': 100686.8125,\n",
       " 'feature_72': 150850.109375,\n",
       " 'feature_73': 106973.1640625,\n",
       " 'feature_74': 50908.7890625,\n",
       " 'feature_75': 75774.875,\n",
       " 'feature_76': 44558.265625,\n",
       " 'feature_77': 34109.20703125,\n",
       " 'feature_78': 59262.70703125,\n",
       " 'feature_79': 36131.5390625,\n",
       " 'feature_80': 96915.828125,\n",
       " 'feature_81': 39850.8984375,\n",
       " 'feature_82': 33218.45703125,\n",
       " 'feature_83': 40862.51171875,\n",
       " 'feature_84': 145521.046875,\n",
       " 'feature_85': 66752.703125,\n",
       " 'feature_86': 72296.625,\n",
       " 'feature_87': 50148.015625,\n",
       " 'feature_88': 40889.10546875,\n",
       " 'feature_89': 70003.84375,\n",
       " 'feature_90': 153668.53125,\n",
       " 'feature_91': 18076.49609375,\n",
       " 'feature_92': 43189.0703125,\n",
       " 'feature_93': 82160.96875,\n",
       " 'feature_94': 63272.19921875,\n",
       " 'feature_95': 108307.9140625,\n",
       " 'feature_96': 83000.6328125,\n",
       " 'feature_97': 73466.109375,\n",
       " 'feature_98': 90529.609375,\n",
       " 'feature_99': 33665.4921875,\n",
       " 'feature_100': 62874.09375,\n",
       " 'feature_101': 57033.5859375,\n",
       " 'feature_102': 50566.28125,\n",
       " 'feature_103': 81093.4140625,\n",
       " 'feature_104': 124147.0234375,\n",
       " 'feature_105': 49809.6484375,\n",
       " 'feature_106': 64893.83203125,\n",
       " 'feature_107': 35158.42578125,\n",
       " 'feature_108': 16242.0986328125,\n",
       " 'feature_109': 45573.36328125,\n",
       " 'feature_110': 35182.70703125,\n",
       " 'feature_111': 65918.203125,\n",
       " 'feature_112': 20264.486328125,\n",
       " 'feature_113': 31262.466796875,\n",
       " 'feature_114': 48708.7734375,\n",
       " 'feature_115': 38333.9140625,\n",
       " 'feature_116': 29829.4375,\n",
       " 'feature_117': 63261.00390625,\n",
       " 'feature_118': 51612.3984375,\n",
       " 'feature_119': 39518.23828125,\n",
       " 'feature_120': 61461.265625,\n",
       " 'feature_121': 65577.0859375,\n",
       " 'feature_122': 125304.796875,\n",
       " 'feature_123': 47497.5625,\n",
       " 'feature_124': 85318.890625,\n",
       " 'feature_125': 81404.8828125,\n",
       " 'feature_126': 62077.4609375,\n",
       " 'feature_127': 52868.20703125,\n",
       " 'feature_128': 62442.48046875,\n",
       " 'feature_129': 40560.953125,\n",
       " 'feature_130': 17525.74609375,\n",
       " 'feature_131': 45535.9140625,\n",
       " 'feature_132': 38743.0703125,\n",
       " 'feature_133': 19312.306640625,\n",
       " 'feature_134': 64106.49609375,\n",
       " 'feature_135': 62434.95703125,\n",
       " 'feature_136': 78487.125,\n",
       " 'feature_137': 83830.7265625,\n",
       " 'feature_138': 55935.08984375,\n",
       " 'feature_139': 312651.5,\n",
       " 'feature_140': 66570.1953125,\n",
       " 'feature_141': 63112.5703125,\n",
       " 'feature_142': 43086.75390625,\n",
       " 'feature_143': 29097.22265625,\n",
       " 'feature_144': 33287.94921875,\n",
       " 'feature_145': 14393.7294921875,\n",
       " 'feature_146': 26825.865234375,\n",
       " 'feature_147': 74120.1015625,\n",
       " 'feature_148': 35222.5,\n",
       " 'feature_149': 82826.671875,\n",
       " 'feature_150': 23120.76171875,\n",
       " 'feature_151': 96437.2734375,\n",
       " 'feature_152': 25763.89453125,\n",
       " 'feature_153': 103305.9140625,\n",
       " 'feature_154': 75839.8046875,\n",
       " 'feature_155': 71572.15625,\n",
       " 'feature_156': 40974.38671875,\n",
       " 'feature_157': 38637.85546875,\n",
       " 'feature_158': 109464.4609375,\n",
       " 'feature_159': 63714.421875,\n",
       " 'feature_160': 31012.1328125,\n",
       " 'feature_161': 49769.27734375,\n",
       " 'feature_162': 66221.7734375,\n",
       " 'feature_163': 68318.9296875,\n",
       " 'feature_164': 14551.134765625,\n",
       " 'feature_165': 54582.25390625,\n",
       " 'feature_166': 40479.109375,\n",
       " 'feature_167': 37573.30078125,\n",
       " 'feature_168': 44192.48046875,\n",
       " 'feature_169': 65894.84375,\n",
       " 'feature_170': 51650.69921875,\n",
       " 'feature_171': 102915.5234375,\n",
       " 'feature_172': 53006.77734375,\n",
       " 'feature_173': 51195.35546875,\n",
       " 'feature_174': 45819.05078125,\n",
       " 'feature_175': 48360.375,\n",
       " 'feature_176': 56554.6796875,\n",
       " 'feature_177': 49067.53125,\n",
       " 'feature_178': 54209.921875,\n",
       " 'feature_179': 97268.0,\n",
       " 'feature_180': 113235.125,\n",
       " 'feature_181': 46436.41015625,\n",
       " 'feature_182': 23574.318359375,\n",
       " 'feature_183': 58491.4609375,\n",
       " 'feature_184': 34153.76953125,\n",
       " 'feature_185': 58592.046875,\n",
       " 'feature_186': 34991.26171875,\n",
       " 'feature_187': 52977.2734375,\n",
       " 'feature_188': 26496.625,\n",
       " 'feature_189': 47261.9375,\n",
       " 'feature_190': 24447.578125,\n",
       " 'feature_191': 19626.107421875,\n",
       " 'feature_192': 34575.29296875,\n",
       " 'feature_193': 32064.599609375,\n",
       " 'feature_194': 91790.9375,\n",
       " 'feature_195': 30154.318359375,\n",
       " 'feature_196': 110078.859375,\n",
       " 'feature_197': 46335.19140625,\n",
       " 'feature_198': 48090.9375,\n",
       " 'feature_199': 31635.19140625,\n",
       " 'feature_200': 52117.828125,\n",
       " 'feature_201': 31477.71875,\n",
       " 'feature_202': 38294.453125,\n",
       " 'feature_203': 29905.708984375,\n",
       " 'feature_204': 43905.98828125,\n",
       " 'feature_205': 101965.125,\n",
       " 'feature_206': 24833.376953125,\n",
       " 'feature_207': 31348.556640625,\n",
       " 'feature_208': 47125.0,\n",
       " 'feature_209': 82089.7265625,\n",
       " 'feature_210': 33080.24609375,\n",
       " 'feature_211': 76099.0625,\n",
       " 'feature_212': 69641.4375,\n",
       " 'feature_213': 50028.47265625,\n",
       " 'feature_214': 36688.38671875,\n",
       " 'feature_215': 26744.123046875,\n",
       " 'feature_216': 70865.109375,\n",
       " 'feature_217': 9861.685546875,\n",
       " 'feature_218': 26686.41015625,\n",
       " 'feature_219': 36693.9296875,\n",
       " 'feature_220': 27578.640625,\n",
       " 'feature_221': 46028.37109375,\n",
       " 'feature_222': 36269.81640625,\n",
       " 'feature_223': 30127.142578125,\n",
       " 'feature_224': 46234.859375,\n",
       " 'feature_225': 59183.28515625,\n",
       " 'feature_226': 63806.16796875,\n",
       " 'feature_227': 57619.0234375,\n",
       " 'feature_228': 38465.08203125,\n",
       " 'feature_229': 16409.5546875,\n",
       " 'feature_230': 28664.62109375,\n",
       " 'feature_231': 59420.32421875,\n",
       " 'feature_232': 24151.8984375,\n",
       " 'feature_233': 55428.56640625,\n",
       " 'feature_234': 35560.52734375,\n",
       " 'feature_235': 19783.326171875,\n",
       " 'feature_236': 42060.4765625,\n",
       " 'feature_237': 31635.421875,\n",
       " 'feature_238': 13521.76953125,\n",
       " 'feature_239': 37678.30078125,\n",
       " 'feature_240': 56698.99609375,\n",
       " 'feature_241': 63515.36328125,\n",
       " 'feature_242': 14918.990234375,\n",
       " 'feature_243': 57067.2578125,\n",
       " 'feature_244': 42505.03515625,\n",
       " 'feature_245': 60270.53515625,\n",
       " 'feature_246': 19521.4609375,\n",
       " 'feature_247': 44900.58984375,\n",
       " 'feature_248': 29935.9765625,\n",
       " 'feature_249': 25201.41796875,\n",
       " 'feature_250': 27658.083984375,\n",
       " 'feature_251': 30691.6171875,\n",
       " 'feature_252': 36926.6328125,\n",
       " 'feature_253': 20526.4921875,\n",
       " 'feature_254': 25875.833984375,\n",
       " 'feature_255': 19424.423828125,\n",
       " 'feature_256': 51545.83984375,\n",
       " 'feature_257': 18724.296875,\n",
       " 'feature_258': 27329.208984375,\n",
       " 'feature_259': 35522.38671875,\n",
       " 'feature_260': 29885.37890625,\n",
       " 'feature_261': 23565.630859375,\n",
       " 'feature_262': 36889.0703125,\n",
       " 'feature_263': 21561.484375,\n",
       " 'feature_264': 30207.3671875,\n",
       " 'feature_265': 42773.59765625,\n",
       " 'feature_266': 28519.708984375,\n",
       " 'feature_267': 24484.880859375,\n",
       " 'feature_268': 37505.37890625,\n",
       " 'feature_269': 81053.9765625,\n",
       " 'feature_270': 63753.171875,\n",
       " 'feature_271': 48779.78515625,\n",
       " 'feature_272': 32354.46484375,\n",
       " 'feature_273': 28040.25,\n",
       " 'feature_274': 41437.79296875,\n",
       " 'feature_275': 29581.4140625,\n",
       " 'feature_276': 43328.90625,\n",
       " 'feature_277': 32700.455078125,\n",
       " 'feature_278': 13041.509765625,\n",
       " 'feature_279': 35987.5234375,\n",
       " 'feature_280': 23759.125,\n",
       " 'feature_281': 40897.79296875,\n",
       " 'feature_282': 53936.0546875,\n",
       " 'feature_283': 30141.455078125,\n",
       " 'feature_284': 48492.734375,\n",
       " 'feature_285': 10536.9921875,\n",
       " 'feature_286': 103578.796875,\n",
       " 'feature_287': 18039.4140625,\n",
       " 'feature_288': 26344.986328125,\n",
       " 'feature_289': 25814.5859375,\n",
       " 'feature_290': 62558.30078125,\n",
       " 'feature_291': 19310.498046875,\n",
       " 'feature_292': 35743.3046875,\n",
       " 'feature_293': 33612.87890625,\n",
       " 'feature_294': 23696.583984375,\n",
       " 'feature_295': 16371.4560546875,\n",
       " 'feature_296': 19750.38671875,\n",
       " 'feature_297': 125070.1953125,\n",
       " 'feature_298': 44779.62109375,\n",
       " 'feature_299': 32400.62890625,\n",
       " 'feature_300': 20625.7421875,\n",
       " 'feature_301': 45240.68359375,\n",
       " 'feature_302': 26970.265625,\n",
       " 'feature_303': 18633.875,\n",
       " 'feature_304': 37855.109375,\n",
       " 'feature_305': 27791.46484375,\n",
       " 'feature_306': 14727.9814453125,\n",
       " 'feature_307': 83811.0,\n",
       " 'feature_308': 18205.55078125,\n",
       " 'feature_309': 14589.015625,\n",
       " 'feature_310': 24763.29296875,\n",
       " 'feature_311': 28987.62109375,\n",
       " 'feature_312': 31563.8828125,\n",
       " 'feature_313': 13300.0068359375,\n",
       " 'feature_314': 27373.51171875,\n",
       " 'feature_315': 34099.13671875,\n",
       " 'feature_316': 14636.26953125,\n",
       " 'feature_317': 18580.77734375,\n",
       " 'feature_318': 28262.16796875,\n",
       " 'feature_319': 51510.4296875,\n",
       " 'feature_320': 24628.962890625,\n",
       " 'feature_321': 80353.8046875,\n",
       " 'feature_322': 22084.134765625,\n",
       " 'feature_323': 32347.052734375,\n",
       " 'feature_324': 24812.296875,\n",
       " 'feature_325': 41611.45703125,\n",
       " 'feature_326': 56319.62890625,\n",
       " 'feature_327': 28615.126953125,\n",
       " 'feature_328': 49960.55078125,\n",
       " 'feature_329': 41955.9921875,\n",
       " 'feature_330': 17892.046875,\n",
       " 'feature_331': 16383.7763671875,\n",
       " 'feature_332': 16394.5,\n",
       " 'feature_333': 12702.3037109375,\n",
       " 'feature_334': 37332.515625,\n",
       " 'feature_335': 26956.01953125,\n",
       " 'feature_336': 23325.01953125,\n",
       " 'feature_337': 24701.837890625,\n",
       " 'feature_338': 43005.73046875,\n",
       " 'feature_339': 31247.103515625,\n",
       " 'feature_340': 48684.2421875,\n",
       " 'feature_341': 19881.314453125,\n",
       " 'feature_342': 45895.48828125,\n",
       " 'feature_343': 41111.96484375,\n",
       " 'feature_344': 22406.234375,\n",
       " 'feature_345': 54143.640625,\n",
       " 'feature_346': 40080.74609375,\n",
       " 'feature_347': 26296.81640625,\n",
       " 'feature_348': 34027.69921875,\n",
       " 'feature_349': 12395.1611328125,\n",
       " 'feature_350': 28585.296875,\n",
       " 'feature_351': 27858.62890625,\n",
       " 'feature_352': 16660.599609375,\n",
       " 'feature_353': 11440.4697265625,\n",
       " 'feature_354': 18978.568359375,\n",
       " 'feature_355': 28560.529296875,\n",
       " 'feature_356': 30339.775390625,\n",
       " 'feature_357': 29415.33203125,\n",
       " 'feature_358': 34362.6796875,\n",
       " 'feature_359': 53075.33203125,\n",
       " 'feature_360': 44047.20703125,\n",
       " 'feature_361': 38265.99609375,\n",
       " 'feature_362': 24725.5625,\n",
       " 'feature_363': 30376.75,\n",
       " 'feature_364': 16990.783203125,\n",
       " 'feature_365': 46943.45703125,\n",
       " 'feature_366': 29180.69140625,\n",
       " 'feature_367': 89222.96875,\n",
       " 'feature_368': 40456.1953125,\n",
       " 'feature_369': 65518.625,\n",
       " 'feature_370': 25389.890625,\n",
       " 'feature_371': 56776.27734375,\n",
       " 'feature_372': 23606.09765625,\n",
       " 'feature_373': 17307.0859375,\n",
       " 'feature_374': 39170.26953125,\n",
       " 'feature_375': 20834.572265625,\n",
       " 'feature_376': 49099.66015625,\n",
       " 'feature_377': 37726.28515625,\n",
       " 'feature_378': 24396.0390625,\n",
       " 'feature_379': 28265.42578125,\n",
       " 'feature_380': 17709.78515625,\n",
       " 'feature_381': 58466.125,\n",
       " 'feature_382': 47713.109375,\n",
       " 'feature_383': 88200.6015625,\n",
       " 'feature_384': 38090.9609375,\n",
       " 'feature_385': 42601.1796875,\n",
       " 'feature_386': 71767.203125,\n",
       " 'feature_387': 21646.8515625,\n",
       " 'feature_388': 34296.421875,\n",
       " 'feature_389': 29884.943359375,\n",
       " 'feature_390': 26020.177734375,\n",
       " 'feature_391': 25145.529296875,\n",
       " 'feature_392': 43203.0078125,\n",
       " 'feature_393': 40319.125,\n",
       " 'feature_394': 21820.154296875,\n",
       " 'feature_395': 32776.3828125,\n",
       " 'feature_396': 34645.4296875,\n",
       " 'feature_397': 44271.53515625,\n",
       " 'feature_398': 18146.685546875,\n",
       " 'feature_399': 50116.14453125,\n",
       " 'feature_400': 39572.15234375,\n",
       " 'feature_401': 38746.5546875,\n",
       " 'feature_402': 25886.375,\n",
       " 'feature_403': 22332.943359375,\n",
       " 'feature_404': 13879.75,\n",
       " 'feature_405': 61726.125,\n",
       " 'feature_406': 18692.71484375,\n",
       " 'feature_407': 16394.533203125,\n",
       " 'feature_408': 33891.86328125,\n",
       " 'feature_409': 25304.7421875,\n",
       " 'feature_410': 30749.470703125,\n",
       " 'feature_411': 15186.8427734375,\n",
       " 'feature_412': 22070.87109375,\n",
       " 'feature_413': 8590.5576171875,\n",
       " 'feature_414': 68466.5234375,\n",
       " 'feature_415': 13856.2421875,\n",
       " 'feature_416': 49256.6328125,\n",
       " 'feature_417': 84949.609375,\n",
       " 'feature_418': 33739.0390625,\n",
       " 'feature_419': 25695.560546875,\n",
       " 'feature_420': 16713.95703125,\n",
       " 'feature_421': 22285.623046875,\n",
       " 'feature_422': 20781.3515625,\n",
       " 'feature_423': 60165.421875,\n",
       " 'feature_424': 17862.0234375,\n",
       " 'feature_425': 16642.123046875,\n",
       " 'feature_426': 27050.421875,\n",
       " 'feature_427': 54599.75,\n",
       " 'feature_428': 26533.6796875,\n",
       " 'feature_429': 17036.42578125,\n",
       " 'feature_430': 19273.9453125,\n",
       " 'feature_431': 35095.16796875,\n",
       " 'feature_432': 61634.15234375,\n",
       " 'feature_433': 15047.0419921875,\n",
       " 'feature_434': 48201.21875,\n",
       " 'feature_435': 30440.66015625,\n",
       " 'feature_436': 31150.0,\n",
       " 'feature_437': 11755.556640625,\n",
       " 'feature_438': 34454.73046875,\n",
       " 'feature_439': 30420.234375,\n",
       " 'feature_440': 58738.3359375,\n",
       " 'feature_441': 32186.861328125,\n",
       " 'feature_442': 31875.384765625,\n",
       " 'feature_443': 21411.197265625,\n",
       " 'feature_444': 26513.298828125,\n",
       " 'feature_445': 33442.0546875,\n",
       " 'feature_446': 104350.8828125,\n",
       " 'feature_447': 20213.859375,\n",
       " 'feature_448': 36499.796875,\n",
       " 'feature_449': 23377.240234375,\n",
       " 'feature_450': 13537.9921875,\n",
       " 'feature_451': 45286.21484375,\n",
       " 'feature_452': 35880.77734375,\n",
       " 'feature_453': 16245.69921875,\n",
       " 'feature_454': 27922.16796875,\n",
       " 'feature_455': 13942.6083984375,\n",
       " 'feature_456': 43132.9375,\n",
       " 'feature_457': 35388.41015625,\n",
       " 'feature_458': 23187.921875,\n",
       " 'feature_459': 39994.10546875,\n",
       " 'feature_460': 31306.107421875,\n",
       " 'feature_461': 31063.087890625,\n",
       " 'feature_462': 42838.9765625,\n",
       " 'feature_463': 32695.376953125,\n",
       " 'feature_464': 22527.70703125,\n",
       " 'feature_465': 22675.626953125,\n",
       " 'feature_466': 37829.3984375,\n",
       " 'feature_467': 58537.01171875,\n",
       " 'feature_468': 44107.1640625,\n",
       " 'feature_469': 31763.75,\n",
       " 'feature_470': 26623.306640625,\n",
       " 'feature_471': 16218.3486328125,\n",
       " 'feature_472': 66807.40625,\n",
       " 'feature_473': 14406.2099609375,\n",
       " 'feature_474': 36820.7578125,\n",
       " 'feature_475': 37156.50390625,\n",
       " 'feature_476': 7785.18017578125,\n",
       " 'feature_477': 75796.0078125,\n",
       " 'feature_478': 29171.765625,\n",
       " 'feature_479': 49456.74609375,\n",
       " 'feature_480': 79805.7890625,\n",
       " 'feature_481': 19811.8125,\n",
       " 'feature_482': 46646.078125,\n",
       " 'feature_483': 84221.90625,\n",
       " 'feature_484': 43084.08984375,\n",
       " 'feature_485': 24291.267578125,\n",
       " 'feature_486': 29725.3671875,\n",
       " 'feature_487': 134481.484375,\n",
       " 'feature_488': 40071.63671875,\n",
       " 'feature_489': 32738.86328125,\n",
       " 'feature_490': 18048.958984375,\n",
       " 'feature_491': 71878.1953125,\n",
       " 'feature_492': 46814.48828125,\n",
       " 'feature_493': 18854.0,\n",
       " 'feature_494': 26725.349609375,\n",
       " 'feature_495': 17876.232421875,\n",
       " 'feature_496': 32116.7109375,\n",
       " 'feature_497': 55450.76953125,\n",
       " 'feature_498': 24081.662109375,\n",
       " 'feature_499': 16000.837890625,\n",
       " 'feature_500': 17184.939453125,\n",
       " 'feature_501': 39194.9296875,\n",
       " 'feature_502': 14655.1826171875,\n",
       " 'feature_503': 31102.462890625,\n",
       " 'feature_504': 120221.734375,\n",
       " 'feature_505': 35509.2578125,\n",
       " 'feature_506': 42593.41015625,\n",
       " 'feature_507': 28586.326171875,\n",
       " 'feature_508': 124156.8671875,\n",
       " 'feature_509': 9394.3251953125,\n",
       " 'feature_510': 27752.1328125,\n",
       " 'feature_511': 34859.23828125,\n",
       " 'feature_512': 45469.359375,\n",
       " 'feature_513': 149907.96875,\n",
       " 'feature_514': 32884.578125,\n",
       " 'feature_515': 35564.5546875,\n",
       " 'feature_516': 63402.22265625,\n",
       " 'feature_517': 30866.068359375,\n",
       " 'feature_518': 22175.716796875,\n",
       " 'feature_519': 31303.921875,\n",
       " 'feature_520': 33714.54296875,\n",
       " 'feature_521': 44066.8125,\n",
       " 'feature_522': 16551.544921875,\n",
       " 'feature_523': 17035.326171875,\n",
       " 'feature_524': 17171.755859375,\n",
       " 'feature_525': 38132.5,\n",
       " 'feature_526': 18571.744140625,\n",
       " 'feature_527': 28246.71484375,\n",
       " 'feature_528': 20979.033203125,\n",
       " 'feature_529': 47598.7890625,\n",
       " 'feature_530': 21227.98046875,\n",
       " 'feature_531': 77214.96875,\n",
       " 'feature_532': 24560.806640625,\n",
       " 'feature_533': 39592.42578125,\n",
       " 'feature_534': 84172.9296875,\n",
       " 'feature_535': 37582.109375,\n",
       " 'feature_536': 29081.79296875,\n",
       " 'feature_537': 12545.3876953125,\n",
       " 'feature_538': 22927.5390625,\n",
       " 'feature_539': 33888.71875,\n",
       " 'feature_540': 8933.03125,\n",
       " 'feature_541': 23168.62109375,\n",
       " 'feature_542': 28184.41015625,\n",
       " 'feature_543': 43488.9453125,\n",
       " 'feature_544': 29445.337890625,\n",
       " 'feature_545': 34289.30859375,\n",
       " 'feature_546': 19874.404296875,\n",
       " 'feature_547': 23029.919921875,\n",
       " 'feature_548': 15831.181640625,\n",
       " 'feature_549': 23746.435546875,\n",
       " 'feature_550': 23410.396484375,\n",
       " 'feature_551': 39271.4921875,\n",
       " 'feature_552': 16936.638671875,\n",
       " 'feature_553': 41138.7734375,\n",
       " 'feature_554': 17816.23828125,\n",
       " 'feature_555': 23454.6484375,\n",
       " 'feature_556': 34938.83203125,\n",
       " 'feature_557': 47581.875,\n",
       " 'feature_558': 37584.828125,\n",
       " 'feature_559': 22705.20703125,\n",
       " 'feature_560': 30197.005859375,\n",
       " 'feature_561': 19342.2890625,\n",
       " 'feature_562': 23691.9453125,\n",
       " 'feature_563': 48362.75,\n",
       " 'feature_564': 31646.58984375,\n",
       " 'feature_565': 51200.47265625,\n",
       " 'feature_566': 28467.3671875,\n",
       " 'feature_567': 40632.98046875,\n",
       " 'feature_568': 10216.8154296875,\n",
       " 'feature_569': 30448.439453125,\n",
       " 'feature_570': 80103.3828125,\n",
       " 'feature_571': 37809.4609375,\n",
       " 'feature_572': 43543.4921875,\n",
       " 'feature_573': 18111.140625,\n",
       " 'feature_574': 33337.18359375,\n",
       " 'feature_575': 21313.73828125,\n",
       " 'feature_576': 15910.9814453125,\n",
       " 'feature_577': 43215.3359375,\n",
       " 'feature_578': 42458.7421875,\n",
       " 'feature_579': 31617.994140625,\n",
       " 'feature_580': 33254.859375,\n",
       " 'feature_581': 25912.3515625,\n",
       " 'feature_582': 27519.880859375,\n",
       " 'feature_583': 16592.18359375,\n",
       " 'feature_584': 20730.96484375,\n",
       " 'feature_585': 77470.4375,\n",
       " 'feature_586': 87971.375,\n",
       " 'feature_587': 24031.990234375,\n",
       " 'feature_588': 21605.55078125,\n",
       " 'feature_589': 24573.857421875,\n",
       " 'feature_590': 46166.265625,\n",
       " 'feature_591': 13973.630859375,\n",
       " 'feature_592': 41009.37109375,\n",
       " 'feature_593': 24759.65625,\n",
       " 'feature_594': 25069.484375,\n",
       " 'feature_595': 37016.97265625,\n",
       " 'feature_596': 88188.84375,\n",
       " 'feature_597': 17978.44140625,\n",
       " 'feature_598': 22510.2421875,\n",
       " 'feature_599': 22257.306640625,\n",
       " 'feature_600': 19242.01171875,\n",
       " 'feature_601': 26524.455078125,\n",
       " 'feature_602': 48326.7109375,\n",
       " 'feature_603': 36624.3359375,\n",
       " 'feature_604': 40285.50390625,\n",
       " 'feature_605': 43970.46875,\n",
       " 'feature_606': 85036.65625,\n",
       " 'feature_607': 19963.025390625,\n",
       " 'feature_608': 29628.1953125,\n",
       " 'feature_609': 32777.13671875,\n",
       " 'feature_610': 15936.95703125,\n",
       " 'feature_611': 158111.828125,\n",
       " 'feature_612': 23750.73046875,\n",
       " 'feature_613': 24113.05859375,\n",
       " 'feature_614': 23661.65234375,\n",
       " 'feature_615': 86055.1015625,\n",
       " 'feature_616': 28811.724609375,\n",
       " 'feature_617': 31012.443359375,\n",
       " 'feature_618': 29138.275390625,\n",
       " 'feature_619': 14376.31640625,\n",
       " 'feature_620': 14410.9755859375,\n",
       " 'feature_621': 24247.36328125,\n",
       " 'feature_622': 31486.806640625,\n",
       " 'feature_623': 19938.091796875,\n",
       " 'feature_624': 30585.81640625,\n",
       " 'feature_625': 38619.62109375,\n",
       " 'feature_626': 21685.701171875,\n",
       " 'feature_627': 45606.6796875,\n",
       " 'feature_628': 10887.931640625,\n",
       " 'feature_629': 62799.67578125,\n",
       " 'feature_630': 22582.095703125,\n",
       " 'feature_631': 25806.9375,\n",
       " 'feature_632': 20880.365234375,\n",
       " 'feature_633': 25683.212890625,\n",
       " 'feature_634': 16694.7421875,\n",
       " 'feature_635': 32302.390625,\n",
       " 'feature_636': 51969.9453125,\n",
       " 'feature_637': 15132.0126953125,\n",
       " 'feature_638': 55438.54296875,\n",
       " 'feature_639': 24888.751953125,\n",
       " 'feature_640': 25802.306640625,\n",
       " 'feature_641': 17671.224609375,\n",
       " 'feature_642': 37775.2890625,\n",
       " 'feature_643': 26792.4921875,\n",
       " 'feature_644': 22386.78125,\n",
       " 'feature_645': 48621.6875,\n",
       " 'feature_646': 25102.59765625,\n",
       " 'feature_647': 23570.546875,\n",
       " 'feature_648': 71010.9765625,\n",
       " 'feature_649': 7833.1220703125,\n",
       " 'feature_650': 18124.541015625,\n",
       " 'feature_651': 36668.0234375,\n",
       " 'feature_652': 26738.03515625,\n",
       " 'feature_653': 38696.22265625,\n",
       " 'feature_654': 20611.2109375,\n",
       " 'feature_655': 26027.5859375,\n",
       " 'feature_656': 25257.001953125,\n",
       " 'feature_657': 17672.67578125,\n",
       " 'feature_658': 18788.662109375,\n",
       " 'feature_659': 30791.0546875,\n",
       " 'feature_660': 62230.98828125,\n",
       " 'feature_661': 87035.3671875,\n",
       " 'feature_662': 16822.37109375,\n",
       " 'feature_663': 11890.4677734375,\n",
       " 'feature_664': 24634.115234375,\n",
       " 'feature_665': 21119.541015625,\n",
       " 'feature_666': 33950.46484375,\n",
       " 'feature_667': 21803.49609375,\n",
       " 'feature_668': 10402.8369140625,\n",
       " 'feature_669': 19181.396484375,\n",
       " 'feature_670': 34684.37109375,\n",
       " 'feature_671': 61236.9140625,\n",
       " 'feature_672': 19215.07421875,\n",
       " 'feature_673': 34543.1328125,\n",
       " 'feature_674': 20816.626953125,\n",
       " 'feature_675': 49326.21484375,\n",
       " 'feature_676': 14797.3916015625,\n",
       " 'feature_677': 30162.568359375,\n",
       " 'feature_678': 43024.765625,\n",
       " 'feature_679': 11494.755859375,\n",
       " 'feature_680': 25565.7265625,\n",
       " 'feature_681': 17039.67578125,\n",
       " 'feature_682': 23458.73046875,\n",
       " 'feature_683': 22808.033203125,\n",
       " 'feature_684': 33796.66015625,\n",
       " 'feature_685': 48807.16796875,\n",
       " 'feature_686': 24657.392578125,\n",
       " 'feature_687': 41898.68359375,\n",
       " 'feature_688': 42893.71875,\n",
       " 'feature_689': 33136.16015625,\n",
       " 'feature_690': 37208.95703125,\n",
       " 'feature_691': 46262.51171875,\n",
       " 'feature_692': 26968.365234375,\n",
       " 'feature_693': 22106.9765625,\n",
       " 'feature_694': 27295.296875,\n",
       " 'feature_695': 24630.240234375,\n",
       " 'feature_696': 25437.23046875,\n",
       " 'feature_697': 19884.87890625,\n",
       " 'feature_698': 28420.76953125,\n",
       " 'feature_699': 51790.26953125,\n",
       " 'feature_700': 85679.578125,\n",
       " 'feature_701': 44968.33984375,\n",
       " 'feature_702': 14744.8203125,\n",
       " 'feature_703': 29669.8203125,\n",
       " 'feature_704': 21409.25,\n",
       " 'feature_705': 25229.072265625,\n",
       " 'feature_706': 16177.3720703125,\n",
       " 'feature_707': 29930.4296875,\n",
       " 'feature_708': 15322.83203125,\n",
       " 'feature_709': 23743.869140625,\n",
       " 'feature_710': 25498.13671875,\n",
       " 'feature_711': 13674.91015625,\n",
       " 'feature_712': 11432.8916015625,\n",
       " 'feature_713': 15322.4560546875,\n",
       " 'feature_714': 20263.416015625,\n",
       " 'feature_715': 12690.9912109375,\n",
       " 'feature_716': 21511.2734375,\n",
       " 'feature_717': 44094.3359375,\n",
       " 'feature_718': 23618.51171875,\n",
       " 'feature_719': 31031.451171875,\n",
       " 'feature_720': 40287.46875,\n",
       " 'feature_721': 74641.3203125,\n",
       " 'feature_722': 29768.388671875,\n",
       " 'feature_723': 14005.986328125,\n",
       " 'feature_724': 41111.60546875,\n",
       " 'feature_725': 24583.1328125,\n",
       " 'feature_726': 26090.62890625,\n",
       " 'feature_727': 60453.3671875,\n",
       " 'feature_728': 63560.1953125,\n",
       " 'feature_729': 71220.046875,\n",
       " 'feature_730': 16726.693359375,\n",
       " 'feature_731': 20124.53515625,\n",
       " 'feature_732': 45235.52734375,\n",
       " 'feature_733': 52704.17578125,\n",
       " 'feature_734': 10538.087890625,\n",
       " 'feature_735': 44762.5234375,\n",
       " 'feature_736': 14162.4765625,\n",
       " 'feature_737': 41832.6875,\n",
       " 'feature_738': 31227.236328125,\n",
       " 'feature_739': 38729.1171875,\n",
       " 'feature_740': 26451.556640625,\n",
       " 'feature_741': 36862.24609375,\n",
       " 'feature_742': 18581.076171875,\n",
       " 'feature_743': 27806.544921875,\n",
       " 'feature_744': 14534.27734375,\n",
       " 'feature_745': 35428.73046875,\n",
       " 'feature_746': 43212.33984375,\n",
       " 'feature_747': 33784.7265625,\n",
       " 'feature_748': 17752.384765625,\n",
       " 'feature_749': 24232.33984375,\n",
       " 'feature_750': 41596.23046875,\n",
       " 'feature_751': 11521.31640625,\n",
       " 'feature_752': 32583.4765625,\n",
       " 'feature_753': 23451.515625,\n",
       " 'feature_754': 27285.833984375,\n",
       " 'feature_755': 31514.08203125,\n",
       " 'feature_756': 60441.04296875,\n",
       " 'feature_757': 19488.326171875,\n",
       " 'feature_758': 18854.62890625,\n",
       " 'feature_759': 61097.1796875,\n",
       " 'feature_760': 15230.0400390625,\n",
       " 'feature_761': 11013.3623046875,\n",
       " 'feature_762': 28059.26171875,\n",
       " 'feature_763': 65240.07421875,\n",
       " 'feature_764': 24935.57421875,\n",
       " 'feature_765': 14094.2587890625,\n",
       " 'feature_766': 27449.978515625,\n",
       " 'feature_767': 40924.09375,\n",
       " 'feature_768': 15791.1484375,\n",
       " 'feature_769': 18377.029296875,\n",
       " 'feature_770': 31540.234375,\n",
       " 'feature_771': 31794.384765625,\n",
       " 'feature_772': 11038.001953125,\n",
       " 'feature_773': 34769.1640625,\n",
       " 'feature_774': 12424.7177734375,\n",
       " 'feature_775': 23638.818359375,\n",
       " 'feature_776': 10591.8603515625,\n",
       " 'feature_777': 79764.5078125,\n",
       " 'feature_778': 36658.23828125,\n",
       " 'feature_779': 31711.82421875,\n",
       " 'feature_780': 34238.63671875,\n",
       " 'feature_781': 18277.33984375,\n",
       " 'feature_782': 9922.0166015625,\n",
       " 'feature_783': 7648.92626953125,\n",
       " 'feature_784': 32333.6015625,\n",
       " 'feature_785': 19608.419921875,\n",
       " 'feature_786': 15998.2255859375,\n",
       " 'feature_787': 91510.609375,\n",
       " 'feature_788': 19247.888671875,\n",
       " 'feature_789': 15824.4189453125,\n",
       " 'feature_790': 48306.63671875,\n",
       " 'feature_791': 23737.619140625,\n",
       " 'feature_792': 25110.193359375,\n",
       " 'feature_793': 23081.830078125,\n",
       " 'feature_794': 15573.3271484375,\n",
       " 'feature_795': 14270.794921875,\n",
       " 'feature_796': 58772.25390625,\n",
       " 'feature_797': 30449.005859375,\n",
       " 'feature_798': 44672.5859375,\n",
       " 'feature_799': 27199.37890625,\n",
       " 'feature_800': 12953.3017578125,\n",
       " 'feature_801': 19000.390625,\n",
       " 'feature_802': 29065.71875,\n",
       " 'feature_803': 18832.927734375,\n",
       " 'feature_804': 20318.05859375,\n",
       " 'feature_805': 18057.837890625,\n",
       " 'feature_806': 13517.19140625,\n",
       " 'feature_807': 90973.3984375,\n",
       " 'feature_808': 39376.1328125,\n",
       " 'feature_809': 63943.40625,\n",
       " 'feature_810': 22082.892578125,\n",
       " 'feature_811': 31145.171875,\n",
       " 'feature_812': 26465.89453125,\n",
       " 'feature_813': 41622.81640625,\n",
       " 'feature_814': 37360.73828125,\n",
       " 'feature_815': 58986.41015625,\n",
       " 'feature_816': 15655.40625,\n",
       " 'feature_817': 23812.7578125,\n",
       " 'feature_818': 21569.302734375,\n",
       " 'feature_819': 36810.78125,\n",
       " 'feature_820': 22054.484375,\n",
       " 'feature_821': 20058.552734375,\n",
       " 'feature_822': 69415.96875,\n",
       " 'feature_823': 16954.00390625,\n",
       " 'feature_824': 20021.671875,\n",
       " 'feature_825': 24953.265625,\n",
       " 'feature_826': 33661.640625,\n",
       " 'feature_827': 20566.5078125,\n",
       " 'feature_828': 29391.37890625,\n",
       " 'feature_829': 38302.96484375,\n",
       " 'feature_830': 26371.572265625,\n",
       " 'feature_831': 12164.01171875,\n",
       " 'feature_832': 16606.48828125,\n",
       " 'feature_833': 69922.515625,\n",
       " 'feature_834': 17357.408203125,\n",
       " 'feature_835': 11720.1064453125,\n",
       " 'feature_836': 43282.2421875,\n",
       " 'feature_837': 13190.3408203125,\n",
       " 'feature_838': 35488.078125,\n",
       " 'feature_839': 36838.49609375,\n",
       " 'feature_840': 37266.9765625,\n",
       " 'feature_841': 40797.9296875,\n",
       " 'feature_842': 32059.955078125,\n",
       " 'feature_843': 30442.810546875,\n",
       " 'feature_844': 29986.310546875,\n",
       " 'feature_845': 30371.658203125,\n",
       " 'feature_846': 47843.5546875,\n",
       " 'feature_847': 17578.73828125,\n",
       " 'feature_848': 48374.5390625,\n",
       " 'feature_849': 42679.4765625,\n",
       " 'feature_850': 28997.4453125,\n",
       " 'feature_851': 34617.8203125,\n",
       " 'feature_852': 16624.080078125,\n",
       " 'feature_853': 30025.1015625,\n",
       " 'feature_854': 41906.9453125,\n",
       " 'feature_855': 19820.05078125,\n",
       " 'feature_856': 79350.828125,\n",
       " 'feature_857': 18948.412109375,\n",
       " 'feature_858': 34454.9765625,\n",
       " 'feature_859': 47008.1171875,\n",
       " 'feature_860': 51422.2265625,\n",
       " 'feature_861': 22701.84765625,\n",
       " 'feature_862': 30418.90234375,\n",
       " 'feature_863': 55805.359375,\n",
       " 'feature_864': 35290.81640625,\n",
       " 'feature_865': 31099.416015625,\n",
       " 'feature_866': 42803.1015625,\n",
       " 'feature_867': 23321.37890625,\n",
       " 'feature_868': 36759.8515625,\n",
       " 'feature_869': 38501.17578125,\n",
       " 'feature_870': 24588.4765625,\n",
       " 'feature_871': 15163.58203125,\n",
       " 'feature_872': 35883.76953125,\n",
       " 'feature_873': 23337.25,\n",
       " 'feature_874': 25687.189453125,\n",
       " 'feature_875': 31292.578125,\n",
       " 'feature_876': 19076.220703125,\n",
       " 'feature_877': 24959.654296875,\n",
       " 'feature_878': 51962.8125,\n",
       " 'feature_879': 30503.296875,\n",
       " 'feature_880': 46651.3203125,\n",
       " 'feature_881': 15284.3720703125,\n",
       " 'feature_882': 35906.3828125,\n",
       " 'feature_883': 69228.2109375,\n",
       " 'feature_884': 49928.234375,\n",
       " 'feature_885': 45526.7734375,\n",
       " 'feature_886': 27933.619140625,\n",
       " 'feature_887': 100991.09375,\n",
       " 'feature_888': 40362.8046875,\n",
       " 'feature_889': 20576.1171875,\n",
       " 'feature_890': 14201.3720703125,\n",
       " 'feature_891': 25126.96484375,\n",
       " 'feature_892': 23103.830078125,\n",
       " 'feature_893': 32564.283203125,\n",
       " 'feature_894': 80308.3125,\n",
       " 'feature_895': 40055.91796875,\n",
       " 'feature_896': 29885.693359375,\n",
       " 'feature_897': 98366.546875,\n",
       " 'feature_898': 16856.30859375,\n",
       " 'feature_899': 30286.884765625,\n",
       " 'feature_900': 33110.1015625,\n",
       " 'feature_901': 25623.98046875,\n",
       " 'feature_902': 26912.48046875,\n",
       " 'feature_903': 103091.3046875,\n",
       " 'feature_904': 33213.375,\n",
       " 'feature_905': 137753.40625,\n",
       " 'feature_906': 76137.3671875,\n",
       " 'feature_907': 32091.396484375,\n",
       " 'feature_908': 33118.796875,\n",
       " 'feature_909': 38735.56640625,\n",
       " 'feature_910': 42797.3828125,\n",
       " 'feature_911': 23738.033203125,\n",
       " 'feature_912': 14331.740234375,\n",
       " 'feature_913': 74838.25,\n",
       " 'feature_914': 20128.314453125,\n",
       " 'feature_915': 35611.296875,\n",
       " 'feature_916': 106856.4765625,\n",
       " 'feature_917': 81416.296875,\n",
       " 'feature_918': 22362.576171875,\n",
       " 'feature_919': 15422.9990234375,\n",
       " 'feature_920': 24142.681640625,\n",
       " 'feature_921': 16862.396484375,\n",
       " 'feature_922': 49730.0234375,\n",
       " 'feature_923': 19816.525390625,\n",
       " 'feature_924': 31561.044921875,\n",
       " 'feature_925': 177680.65625,\n",
       " 'feature_926': 67679.0,\n",
       " 'feature_927': 17649.654296875,\n",
       " 'feature_928': 27174.923828125,\n",
       " 'feature_929': 63195.25,\n",
       " 'feature_930': 29321.96875,\n",
       " 'feature_931': 37341.046875,\n",
       " 'feature_932': 12976.1640625,\n",
       " 'feature_933': 43872.66015625,\n",
       " 'feature_934': 21821.580078125,\n",
       " 'feature_935': 12234.9453125,\n",
       " 'feature_936': 33791.0703125,\n",
       " 'feature_937': 45957.00390625,\n",
       " 'feature_938': 36903.2265625,\n",
       " 'feature_939': 29995.703125,\n",
       " 'feature_940': 18121.65234375,\n",
       " 'feature_941': 23951.12890625,\n",
       " 'feature_942': 30211.14453125,\n",
       " 'feature_943': 27331.080078125,\n",
       " 'feature_944': 49040.62109375,\n",
       " 'feature_945': 22106.8984375,\n",
       " 'feature_946': 24657.580078125,\n",
       " 'feature_947': 23031.439453125,\n",
       " 'feature_948': 24063.4609375,\n",
       " 'feature_949': 21010.341796875,\n",
       " 'feature_950': 36326.8359375,\n",
       " 'feature_951': 57727.3359375,\n",
       " 'feature_952': 34254.01171875,\n",
       " 'feature_953': 18354.587890625,\n",
       " 'feature_954': 20342.97265625,\n",
       " 'feature_955': 9984.5400390625,\n",
       " 'feature_956': 10287.57421875,\n",
       " 'feature_957': 83051.0703125,\n",
       " 'feature_958': 32688.115234375,\n",
       " 'feature_959': 19018.0390625,\n",
       " 'feature_960': 24782.47265625,\n",
       " 'feature_961': 11595.4853515625,\n",
       " 'feature_962': 28769.73046875,\n",
       " 'feature_963': 43985.96484375,\n",
       " 'feature_964': 19230.076171875,\n",
       " 'feature_965': 18138.01953125,\n",
       " 'feature_966': 38120.171875,\n",
       " 'feature_967': 20807.07421875,\n",
       " 'feature_968': 39354.96875,\n",
       " 'feature_969': 41353.96875,\n",
       " 'feature_970': 47183.03125,\n",
       " 'feature_971': 40818.34765625,\n",
       " 'feature_972': 42905.5625,\n",
       " 'feature_973': 41917.58984375,\n",
       " 'feature_974': 24158.94921875,\n",
       " 'feature_975': 20985.47265625,\n",
       " 'feature_976': 80706.3671875,\n",
       " 'feature_977': 46490.25,\n",
       " 'feature_978': 116512.609375,\n",
       " 'feature_979': 56342.42578125,\n",
       " 'feature_980': 21339.853515625,\n",
       " 'feature_981': 15370.19921875,\n",
       " 'feature_982': 19732.09375,\n",
       " 'feature_983': 68519.3515625,\n",
       " 'feature_984': 28436.494140625,\n",
       " 'feature_985': 104952.03125,\n",
       " 'feature_986': 28415.71875,\n",
       " 'feature_987': 26328.28515625,\n",
       " 'feature_988': 48646.9921875,\n",
       " 'feature_989': 60674.0078125,\n",
       " 'feature_990': 16839.447265625,\n",
       " 'feature_991': 33868.875,\n",
       " 'feature_992': 27108.3515625,\n",
       " 'feature_993': 28740.01953125,\n",
       " 'feature_994': 35762.36328125,\n",
       " 'feature_995': 23601.564453125,\n",
       " 'feature_996': 45159.94140625,\n",
       " 'feature_997': 33099.5703125,\n",
       " 'feature_998': 28815.615234375,\n",
       " 'feature_999': 33909.99609375}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.get_booster().get_score(importance_type='gain')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
